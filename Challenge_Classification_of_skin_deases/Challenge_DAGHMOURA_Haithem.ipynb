{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1fAqwjrSkwf",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Packages version and Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "Ak-bINK18U28"
      },
      "source": [
        "## The Packages versions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qtl_TVsi8U2-"
      },
      "outputs": [],
      "source": [
        "# Package                      Version\n",
        "# ---------------------------- --------------\n",
        "# absl-py                      2.1.0\n",
        "# anyio                        4.3.0\n",
        "# argon2-cffi                  23.1.0\n",
        "# argon2-cffi-bindings         21.2.0\n",
        "# arrow                        1.3.0\n",
        "# asttokens                    2.4.1\n",
        "# astunparse                   1.6.3\n",
        "# async-lru                    2.0.4\n",
        "# attrs                        23.2.0\n",
        "# autokeras                    1.1.0\n",
        "# Babel                        2.14.0\n",
        "# backcall                     0.2.0\n",
        "# beautifulsoup4               4.12.3\n",
        "# bleach                       6.1.0\n",
        "# cachetools                   5.3.3\n",
        "# certifi                      2024.2.2\n",
        "# cffi                         1.16.0\n",
        "# charset-normalizer           3.3.2\n",
        "# comm                         0.2.2\n",
        "# contourpy                    1.1.1\n",
        "# cycler                       0.12.1\n",
        "# debugpy                      1.8.1\n",
        "# decorator                    5.1.1\n",
        "# defusedxml                   0.7.1\n",
        "# dm-tree                      0.1.8\n",
        "# exceptiongroup               1.2.1\n",
        "# executing                    2.0.1\n",
        "# fastjsonschema               2.19.1\n",
        "# filelock                     3.13.4\n",
        "# flatbuffers                  24.3.25\n",
        "# fonttools                    4.51.0\n",
        "# fqdn                         1.5.1\n",
        "# fsspec                       2024.3.1\n",
        "# gast                         0.4.0\n",
        "# google-auth                  2.29.0\n",
        "# google-auth-oauthlib         1.0.0\n",
        "# google-pasta                 0.2.0\n",
        "# grpcio                       1.62.2\n",
        "# h11                          0.14.0\n",
        "# h5py                         3.11.0\n",
        "# httpcore                     1.0.5\n",
        "# httpx                        0.27.0\n",
        "# idna                         3.7\n",
        "# imbalanced-learn             0.12.2\n",
        "# imblearn                     0.0\n",
        "# importlib_metadata           7.1.0\n",
        "# importlib_resources          6.4.0\n",
        "# ipykernel                    6.29.4\n",
        "# ipython                      8.12.3\n",
        "# ipywidgets                   8.1.2\n",
        "# isoduration                  20.11.0\n",
        "# jedi                         0.19.1\n",
        "# Jinja2                       3.1.3\n",
        "# joblib                       1.4.0\n",
        "# json5                        0.9.25\n",
        "# jsonpointer                  2.4\n",
        "# jsonschema                   4.21.1\n",
        "# jsonschema-specifications    2023.12.1\n",
        "# jupyter                      1.0.0\n",
        "# jupyter_client               8.6.1\n",
        "# jupyter-console              6.6.3\n",
        "# jupyter_core                 5.7.2\n",
        "# jupyter-events               0.10.0\n",
        "# jupyter-lsp                  2.2.5\n",
        "# jupyter_server               2.14.0\n",
        "# jupyter_server_terminals     0.5.3\n",
        "# jupyterlab                   4.1.6\n",
        "# jupyterlab_pygments          0.3.0\n",
        "# jupyterlab_server            2.27.0\n",
        "# jupyterlab_widgets           3.0.10\n",
        "# keras                        2.13.1\n",
        "# keras-core                   0.1.5\n",
        "# keras-nlp                    0.6.1\n",
        "# keras-tuner                  1.4.7\n",
        "# kiwisolver                   1.4.5\n",
        "# kt-legacy                    1.0.5\n",
        "# libclang                     18.1.1\n",
        "# Markdown                     3.6\n",
        "# markdown-it-py               3.0.0\n",
        "# MarkupSafe                   2.1.5\n",
        "# matplotlib                   3.7.5\n",
        "# matplotlib-inline            0.1.7\n",
        "# mdurl                        0.1.2\n",
        "# mistune                      3.0.2\n",
        "# mpmath                       1.3.0\n",
        "# namex                        0.0.8\n",
        "# nbclient                     0.10.0\n",
        "# nbconvert                    7.16.3\n",
        "# nbformat                     5.10.4\n",
        "# nest-asyncio                 1.6.0\n",
        "# networkx                     3.1\n",
        "# notebook                     7.1.3\n",
        "# notebook_shim                0.2.4\n",
        "# np-utils                     0.6.0\n",
        "# numpy                        1.24.3\n",
        "# nvidia-cublas-cu12           12.1.3.1\n",
        "# nvidia-cuda-cupti-cu12       12.1.105\n",
        "# nvidia-cuda-nvrtc-cu12       12.1.105\n",
        "# nvidia-cuda-runtime-cu12     12.1.105\n",
        "# nvidia-cudnn-cu12            8.9.2.26\n",
        "# nvidia-cufft-cu12            11.0.2.54\n",
        "# nvidia-curand-cu12           10.3.2.106\n",
        "# nvidia-cusolver-cu12         11.4.5.107\n",
        "# nvidia-cusparse-cu12         12.1.0.106\n",
        "# nvidia-nccl-cu12             2.19.3\n",
        "# nvidia-nvjitlink-cu12        12.4.127\n",
        "# nvidia-nvtx-cu12             12.1.105\n",
        "# oauthlib                     3.2.2\n",
        "# opencv-python                4.9.0.80\n",
        "# opt-einsum                   3.3.0\n",
        "# overrides                    7.7.0\n",
        "# packaging                    24.0\n",
        "# pandas                       2.0.3\n",
        "# pandocfilters                1.5.1\n",
        "# parso                        0.8.4\n",
        "# pexpect                      4.9.0\n",
        "# pickleshare                  0.7.5\n",
        "# pillow                       10.3.0\n",
        "# pip                          23.3.1\n",
        "# pkgutil_resolve_name         1.3.10\n",
        "# platformdirs                 4.2.0\n",
        "# prometheus_client            0.20.0\n",
        "# prompt-toolkit               3.0.43\n",
        "# protobuf                     4.25.3\n",
        "# psutil                       5.9.8\n",
        "# ptyprocess                   0.7.0\n",
        "# pure-eval                    0.2.2\n",
        "# pyasn1                       0.6.0\n",
        "# pyasn1_modules               0.4.0\n",
        "# pycparser                    2.22\n",
        "# Pygments                     2.17.2\n",
        "# pyparsing                    3.1.2\n",
        "# python-dateutil              2.9.0.post0\n",
        "# python-json-logger           2.0.7\n",
        "# pytz                         2024.1\n",
        "# PyYAML                       6.0.1\n",
        "# pyzmq                        26.0.2\n",
        "# qtconsole                    5.5.1\n",
        "# QtPy                         2.4.1\n",
        "# referencing                  0.34.0\n",
        "# regex                        2024.4.16\n",
        "# requests                     2.31.0\n",
        "# requests-oauthlib            2.0.0\n",
        "# rfc3339-validator            0.1.4\n",
        "# rfc3986-validator            0.1.1\n",
        "# rich                         13.7.1\n",
        "# rpds-py                      0.18.0\n",
        "# rsa                          4.9\n",
        "# scikit-learn                 1.3.2\n",
        "# scipy                        1.10.1\n",
        "# seaborn                      0.13.2\n",
        "# Send2Trash                   1.8.3\n",
        "# setuptools                   68.2.2\n",
        "# six                          1.16.0\n",
        "# sniffio                      1.3.1\n",
        "# soupsieve                    2.5\n",
        "# stack-data                   0.6.3\n",
        "# sympy                        1.12\n",
        "# tensorboard                  2.13.0\n",
        "# tensorboard-data-server      0.7.2\n",
        "# tensorflow                   2.13.1\n",
        "# tensorflow-estimator         2.13.0\n",
        "# tensorflow-hub               0.16.1\n",
        "# tensorflow-io-gcs-filesystem 0.34.0\n",
        "# tensorflow-text              2.13.0\n",
        "# termcolor                    2.4.0\n",
        "# terminado                    0.18.1\n",
        "# tf-keras                     2.15.0\n",
        "# threadpoolctl                3.4.0\n",
        "# tinycss2                     1.2.1\n",
        "# tomli                        2.0.1\n",
        "# torch                        2.2.2\n",
        "# torchvision                  0.17.2\n",
        "# tornado                      6.4\n",
        "# tqdm                         4.66.2\n",
        "# traitlets                    5.14.3\n",
        "# triton                       2.2.0\n",
        "# types-python-dateutil        2.9.0.20240316\n",
        "# typing_extensions            4.5.0\n",
        "# tzdata                       2024.1\n",
        "# uri-template                 1.3.0\n",
        "# urllib3                      2.2.1\n",
        "# wcwidth                      0.2.13\n",
        "# webcolors                    1.13\n",
        "# webencodings                 0.5.1\n",
        "# websocket-client             1.7.0\n",
        "# Werkzeug                     3.0.2\n",
        "# wheel                        0.41.2\n",
        "# widgetsnbextension           4.0.10\n",
        "# wrapt                        1.16.0\n",
        "# zipp                         3.18.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "yvvUTHcd8U3N"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TL0QfXeSmMV",
        "outputId": "37851a06-80bf-48b6-ae20-68751ab4f624"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2.2+cu121\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import csv\n",
        "import shutil\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torchvision.transforms as tt\n",
        "import torch\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split,TensorDataset,Subset,TensorDataset, DataLoader\n",
        "import random\n",
        "import cv2\n",
        "import datetime\n",
        "from sklearn.model_selection import StratifiedShuffleSplit,train_test_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "import sklearn  # scikit-learn\n",
        "# import pytorch modules\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import torchvision.models\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwy3eCBrQxGW",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Unzipping the \"ima205-challenge-2024.zip file\"\n",
        "It's the same file that we can download from the competetion in kaggle\n",
        "To delete if the file already exists in the same directory as the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "au7SAWYtQf3N"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Specify the path to the zip file\n",
        "zip_file_path = \"ima205-challenge-2024.zip\"\n",
        "\n",
        "# Specify the directory where you want to extract the contents\n",
        "extract_to_directory = \"./\"\n",
        "\n",
        "# Create the extraction directory if it doesn't exist\n",
        "os.makedirs(extract_to_directory, exist_ok=True)\n",
        "\n",
        "# Open the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    # Extract all contents to the extraction directory\n",
        "    zip_ref.extractall(extract_to_directory)\n",
        "\n",
        "# Function to recursively extract nested directories\n",
        "def extract_nested_directories(directory):\n",
        "    for item in os.listdir(directory):\n",
        "        item_path = os.path.join(directory, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            extract_nested_directories(item_path)\n",
        "        elif zipfile.is_zipfile(item_path):\n",
        "            with zipfile.ZipFile(item_path, 'r') as zip_ref:\n",
        "                zip_ref.extractall(directory)\n",
        "\n",
        "# Call the function to extract nested directories\n",
        "extract_nested_directories(extract_to_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Pz2KqjHO_aJ",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Organizing files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE0Z6povY8An",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Seperating images and masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62JTipCPWAj4"
      },
      "outputs": [],
      "source": [
        "def move_files_with_pattern(source_dir, dest_dir):\n",
        "    # Create the destination directory if it doesn't exist\n",
        "    os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "    # List all files in the source directory\n",
        "    files = os.listdir(source_dir)\n",
        "\n",
        "    # Iterate over each file\n",
        "    for file in files:\n",
        "        # Check if the file name matches the pattern\n",
        "        if  file.endswith('.png'):\n",
        "            # Construct the source and destination paths\n",
        "            src_path = os.path.join(source_dir, file)\n",
        "            dest_path = os.path.join(dest_dir, file)\n",
        "\n",
        "            # Move the file to the destination directory\n",
        "            shutil.move(src_path, dest_path)\n",
        "            #print(f\"Moved file: {file}\")\n",
        "\n",
        "# Sperating test images and test masks\n",
        "    # Specify the source directory containing the images\n",
        "source_directory = './Test/Test'\n",
        "\n",
        "    # Specify the destination directory for files matching the pattern\n",
        "destination_directory = './Test_masks'\n",
        "\n",
        "    # Call the function to move files matching the pattern\n",
        "move_files_with_pattern(source_directory, destination_directory)\n",
        "\n",
        "\n",
        "# Seperating training images and training masks\n",
        "    # Specify the source directory containing the images\n",
        "source_directory = './Train/Train'\n",
        "\n",
        "    # Specify the destination directory for files matching the pattern\n",
        "destination_directory = './Train_masks'\n",
        "\n",
        "    # Call the function to move files matching the pattern\n",
        "move_files_with_pattern(source_directory, destination_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rKwpidVWAq9",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Creating seperate file for each class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDCKxk0DWAlt"
      },
      "outputs": [],
      "source": [
        "# Path to the folder containing images\n",
        "images_folder = './Train/Train'\n",
        "new_folder_path = './Train_Valid'\n",
        "os.makedirs(new_folder_path, exist_ok=True)\n",
        "\n",
        "# Path to the CSV file\n",
        "csv_file = './metadataTrain.csv'\n",
        "\n",
        "# Create folders for each class\n",
        "for class_num in range(1, 9):\n",
        "    class_folder = os.path.join(new_folder_path, f'class_{class_num}')\n",
        "    os.makedirs(class_folder, exist_ok=True)\n",
        "\n",
        "# Read the CSV file and move images to respective folders\n",
        "with open(csv_file, 'r') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        image_name = row['ID']+'.jpg'  # assuming 'image_name' is the column name for image names\n",
        "        class_num = int(row['CLASS'])  # assuming 'class' is the column name for class numbers\n",
        "\n",
        "        # Move the image to the corresponding class folder\n",
        "        src_path = os.path.join(images_folder, image_name)\n",
        "        dest_path = os.path.join(new_folder_path, f'class_{class_num}', image_name)\n",
        "        shutil.move(src_path, dest_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DR6vAFWq8U3S",
        "outputId": "3087920a-fa3b-49af-fc1e-ce7308fdfd41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory removed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Directory to be removed\n",
        "directory_to_remove = \"./Train\"\n",
        "\n",
        "# Remove the directory and its contents recursively\n",
        "shutil.rmtree(directory_to_remove)\n",
        "\n",
        "print(\"Directory removed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1s1wLsFaL7e",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Splitting into Train-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcu9hH_qaPw3"
      },
      "outputs": [],
      "source": [
        "def split_dataset(source_dir, train_dir, val_dir, split_ratio=0.8):\n",
        "    # Create train and validation directories if they don't exist\n",
        "    os.makedirs(train_dir, exist_ok=True)\n",
        "    os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "    # Iterate through each folder in the source directory\n",
        "    for folder_name in os.listdir(source_dir):\n",
        "        folder_path = os.path.join(source_dir, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            # Create train and validation subdirectories for the current folder\n",
        "            train_folder_path = os.path.join(train_dir, folder_name)\n",
        "            val_folder_path = os.path.join(val_dir, folder_name)\n",
        "            os.makedirs(train_folder_path, exist_ok=True)\n",
        "            os.makedirs(val_folder_path, exist_ok=True)\n",
        "\n",
        "            # Get list of files in the current folder\n",
        "            files = os.listdir(folder_path)\n",
        "            random.shuffle(files)  # Shuffle the list of files\n",
        "\n",
        "            # Calculate the number of files to keep in the original folder\n",
        "            split_index = int(len(files) * split_ratio)\n",
        "            train_files = files[:split_index]\n",
        "            val_files = files[split_index:]\n",
        "\n",
        "            # Move files to train directory\n",
        "            for file in train_files:\n",
        "                src = os.path.join(folder_path, file)\n",
        "                dst = os.path.join(train_folder_path, file)\n",
        "                shutil.move(src, dst)\n",
        "\n",
        "            # Move files to validation directory\n",
        "            for file in val_files:\n",
        "                src = os.path.join(folder_path, file)\n",
        "                dst = os.path.join(val_folder_path, file)\n",
        "                shutil.move(src, dst)\n",
        "\n",
        "\n",
        "source_dir = 'Train_Valid'\n",
        "train_dir = 'Train'\n",
        "val_dir = 'Validation'\n",
        "split_ratio = 0.8\n",
        "split_dataset(source_dir, train_dir, val_dir, split_ratio)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7GEhmDCaPsw",
        "outputId": "c83f977c-4590-409c-c8b6-6565fb025a83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Directory removed successfully.\n"
          ]
        }
      ],
      "source": [
        "# Directory to be removed\n",
        "directory_to_remove = \"./Train_Valid\"\n",
        "\n",
        "# Remove the directory and its contents recursively\n",
        "shutil.rmtree(directory_to_remove)\n",
        "\n",
        "print(\"Directory removed successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVmB37BAZHKH",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBncVM7FZHDP"
      },
      "outputs": [],
      "source": [
        "Train_dir = \"Train\"\n",
        "Valid_dir = \"Validation\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoJZahNWbiTI",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Dealing with black areas in the images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ldbCwhHbllH",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Determining the the images to crop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfgrI5e9ZHBD"
      },
      "outputs": [],
      "source": [
        "def check_border_value(image_path, threshold=10):\n",
        "    # Load the image\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Get image dimensions\n",
        "    height, width, _ = img.shape\n",
        "\n",
        "    # Define the border width (4 pixels)\n",
        "    border_width = 10\n",
        "\n",
        "    # Check top and bottom borders\n",
        "    for i in range(border_width):\n",
        "        top_row_values = img[i, :, 0]  # Get pixel values for the ith row\n",
        "        bottom_row_values = img[height - 1 - i, :, 0]  # Get pixel values for the (height-1-i)th row\n",
        "        if any(value > threshold for value in top_row_values) or any(value > threshold for value in bottom_row_values):\n",
        "            return 0\n",
        "\n",
        "    # Check left and right borders (excluding corners already checked)\n",
        "    for i in range(border_width, height - border_width):\n",
        "        left_column_values = img[i, 0:border_width, 0]  # Get pixel values for the first border_width columns\n",
        "        right_column_values = img[i, width - border_width:, 0]  # Get pixel values for the last border_width columns\n",
        "        if any(value > threshold for value in left_column_values) or any(value > threshold for value in right_column_values):\n",
        "            return 0\n",
        "\n",
        "    return 1\n",
        "\n",
        "# Example usage:\n",
        "# image_path = os.path.join(Train_dir,\"class_1\",'ISIC_0070246.jpg')\n",
        "# result = check_border_value(image_path)\n",
        "# print(\"Border value check result:\", result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1CoGAiOb7ro",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Cropping the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRdcnqZwbxF_"
      },
      "outputs": [],
      "source": [
        "def crop_circular_region(image_path):\n",
        "    \"\"\" cropps the current image to remove the at best the black contour\"\"\"\n",
        "\n",
        "    # Load the image\n",
        "    img = cv2.imread(image_path)\n",
        "\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    # Thresholding to separate black contour\n",
        "    _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    # Find contours\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Filter contours based on size and shape\n",
        "    max_contour = max(contours, key=cv2.contourArea)\n",
        "\n",
        "    perimeter = cv2.arcLength(max_contour, True)\n",
        "    approx = cv2.approxPolyDP(max_contour, 0.02 * perimeter, True)\n",
        "    cropped_img = img\n",
        "    # If contour is approximately circular\n",
        "    if len(approx) >= 7:\n",
        "        # Calculate bounding box around circular region\n",
        "        x, y, w, h = cv2.boundingRect(max_contour)\n",
        "        # Crop the image using bounding box coordinates\n",
        "        cropped_img = img[y:y+h, x:x+w]\n",
        "\n",
        "        return cropped_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cowS__Wwb2U3"
      },
      "outputs": [],
      "source": [
        "def process_images(directory):\n",
        "    # Iterate through all files and directories in the given directory\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        #print(root,dirs,files)\n",
        "        for filename in files:\n",
        "           if filename.endswith(('.jpg')):\n",
        "                 image_path = os.path.join(root, filename)\n",
        "                 if check_border_value(image_path):\n",
        "                        #print(\"cropping image \"+image_path)\n",
        "                        cv2.imwrite(image_path, crop_circular_region(image_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWpeMc7kb2Sr",
        "outputId": "e189bf8a-e948-4290-9adc-fc28239e9f17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "done\n"
          ]
        }
      ],
      "source": [
        "directory = 'Train'\n",
        "process_images(directory)\n",
        "\n",
        "directory = 'Validation'\n",
        "process_images(directory)\n",
        "\n",
        "directory = 'Test'\n",
        "process_images(directory)\n",
        "print(\"done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYd468r8gr2K",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Verifying contents of each directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Sd4xCzCcHt3",
        "outputId": "16344ff5-9710-45b0-95bd-c30bf7438492"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "for Training Folder we have:\n",
            "\n",
            "Directory 'class_5' has 1574 files.\n",
            "Directory 'class_2' has 7725 files.\n",
            "Directory 'class_8' has 376 files.\n",
            "Directory 'class_6' has 143 files.\n",
            "Directory 'class_1' has 2712 files.\n",
            "Directory 'class_3' has 1993 files.\n",
            "Directory 'class_4' has 520 files.\n",
            "Directory 'class_7' has 152 files.\n",
            "\n",
            "\n",
            "for validation Folder we have:\n",
            "\n",
            "Directory 'class_5' has 394 files.\n",
            "Directory 'class_8' has 95 files.\n",
            "Directory 'class_2' has 1932 files.\n",
            "Directory 'class_6' has 36 files.\n",
            "Directory 'class_1' has 679 files.\n",
            "Directory 'class_3' has 499 files.\n",
            "Directory 'class_4' has 130 files.\n",
            "Directory 'class_7' has 38 files.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def count_files_in_folders(path_dir):\n",
        "    # Get the current directory\n",
        "    current_directory = path_dir\n",
        "\n",
        "    # Iterate over directories in the current directory\n",
        "    for directory_name in os.listdir(current_directory):\n",
        "        current_path = os.path.join(current_directory,directory_name)\n",
        "        if os.path.isdir(current_path):\n",
        "            # Count the number of files in the directory\n",
        "            num_files = len([filename for filename in os.listdir(current_path) if os.path.isfile(os.path.join(current_path, filename))])\n",
        "            print(f\"Directory '{directory_name}' has {num_files} files.\")\n",
        "\n",
        "print(\"for Training Folder we have:\\n\")\n",
        "count_files_in_folders(\"Train\")\n",
        "print(\"\\n\\nfor validation Folder we have:\\n\")\n",
        "count_files_in_folders(\"Validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nWZ9EoRZKpm",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndyUrXfAZKJP",
        "outputId": "2dba9731-1e4d-4513-a6d7-312bd73e09eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-04-28 01:04:07.815802: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-28 01:04:18.706373: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['class_5', 'class_2', 'class_8', 'class_6', 'class_1', 'class_3', 'class_4', 'class_7']\n",
            "7725\n",
            "Train/class_5  \n",
            "number needed :  6151\n",
            "\n",
            "done with directoryTrain/class_5\n",
            "4618\n",
            "Train/class_8  \n",
            "number needed :  7349\n",
            "\n",
            "done with directoryTrain/class_8\n",
            "5239\n",
            "Train/class_6  \n",
            "number needed :  7582\n",
            "\n",
            "done with directoryTrain/class_6\n",
            "5296\n",
            "Train/class_1  \n",
            "number needed :  5013\n",
            "\n",
            "done with directoryTrain/class_1\n",
            "3912\n",
            "Train/class_3  \n",
            "number needed :  5732\n",
            "\n",
            "done with directoryTrain/class_3\n",
            "4351\n",
            "Train/class_4  \n",
            "number needed :  7205\n",
            "\n",
            "done with directoryTrain/class_4\n",
            "5121\n",
            "Train/class_7  \n",
            "number needed :  7573\n",
            "\n",
            "done with directoryTrain/class_7\n",
            "5340\n",
            "Data augmentation completed. Here is the new class counts in the training set\n",
            "\n",
            "[6192, 7725, 5615, 5439, 6624, 6344, 5641, 5492]\n"
          ]
        }
      ],
      "source": [
        "# Define the paths to the TrainValid folder and its subfolders\n",
        "train_folder = \"Train\"\n",
        "subfolders = os.listdir(train_folder)\n",
        "print(subfolders)\n",
        "\n",
        "# Find the maximum number of images among all subfolders\n",
        "max_num_images = max([len(os.listdir(os.path.join(train_folder, folder))) for folder in subfolders])\n",
        "print(max_num_images)\n",
        "\n",
        "# Define the data augmentation parameters\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    brightness_range=[0.9, 1.1],\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "# Iterate through each subfolder and perform data augmentation\n",
        "for folder in subfolders:\n",
        "    folder_path = os.path.join(train_folder, folder)\n",
        "    num_images = len(os.listdir(folder_path))\n",
        "    num_augmented_images_needed = max_num_images - num_images\n",
        "\n",
        "    # If the folder already has the maximum number of images, continue to the next folder\n",
        "    if num_augmented_images_needed <= 0:\n",
        "        continue\n",
        "\n",
        "    # Create a temporary directory to store augmented images\n",
        "    temp_dir = os.path.join(folder_path, \"temp\")\n",
        "    os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "    # Iterate through each image in the folder and perform data augmentation\n",
        "    image_files = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path)]\n",
        "    #print(image_files)\n",
        "    num_image_generated =0\n",
        "    print(folder_path,\" \\nnumber needed : \", num_augmented_images_needed)\n",
        "    for image_file in image_files:\n",
        "        if(image_file[-3:] != \"jpg\"):\n",
        "          continue\n",
        "        img = load_img(image_file)\n",
        "        x = img_to_array(img)\n",
        "        x = x.reshape((1,) + x.shape)\n",
        "\n",
        "        if(num_image_generated>=num_augmented_images_needed):\n",
        "                break\n",
        "        num_per_image =np.ceil( num_augmented_images_needed/num_images)\n",
        "        # Generate augmented images\n",
        "        i=0\n",
        "        for batch in datagen.flow(x, batch_size=1, save_to_dir=temp_dir, save_prefix='aug__', save_format='jpg'):\n",
        "            if(num_image_generated>=num_augmented_images_needed):\n",
        "                break\n",
        "            i += 1\n",
        "            num_image_generated+=1\n",
        "            if i >( num_per_image):\n",
        "                break\n",
        "\n",
        "           # print(i,\" - \",len(os.listdir(temp_dir)))\n",
        "    print(\"\\ndone with directory\"+folder_path)\n",
        "    # Move augmented images from the temporary directory to the original folder\n",
        "    augmented_images = [os.path.join(temp_dir, filename) for filename in os.listdir(temp_dir)]\n",
        "    #print(len(augmented_images))\n",
        "    for augmented_image in augmented_images:\n",
        "        shutil.move(augmented_image, folder_path)\n",
        "\n",
        "    # Remove the temporary directory\n",
        "    shutil.rmtree(temp_dir)\n",
        "\n",
        "\n",
        "print(\"Data augmentation completed. Here is the new class counts in the training set\\n\")\n",
        "print([len(os.listdir(os.path.join(train_folder, folder))) for folder in subfolders])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A9-j8HQiwCT"
      },
      "source": [
        "# Training the model : ResNet9 architecture and predicting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7baBeWFxj0kJ",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_UvJX-PjDyx",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Loading the Train set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V91D7BI9ZG-q",
        "outputId": "836e82bc-25d6-4284-d1b3-71b61e8ff57d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nbre of samples in training set:  49072 \n",
            "\n",
            "here are the classes distribution :\n",
            "\n",
            "class_1 : 6624\n",
            "class_2 : 7725\n",
            "class_3 : 6344\n",
            "class_4 : 5641\n",
            "class_5 : 6192\n",
            "class_6 : 5439\n",
            "class_7 : 5492\n",
            "class_8 : 5615\n"
          ]
        }
      ],
      "source": [
        "Train_data_dir ='Train'\n",
        "Train_dataset = ImageFolder(root=Train_data_dir, transform=tt.Compose([tt.Resize((128,128)), #Resizes each image\n",
        "                                            tt.ToTensor() ])) #Converts the image to a PyTorch tensor\n",
        "\n",
        "Train_nbre_samples=len(Train_dataset)\n",
        "print(f\"nbre of samples in training set:  {len(Train_dataset)} \\n\" )\n",
        "print(\"here are the classes distribution :\\n\")\n",
        "count_class = []\n",
        "for cls in sorted(os.listdir(Train_data_dir)):\n",
        "  print(cls, ':', len(os.listdir(Train_data_dir + '/' + cls)))\n",
        "  count_class.append(len(os.listdir(Train_data_dir + '/' + cls)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yf7qHCLdjGN5",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Loading the validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLYm7ERIZG7i",
        "outputId": "42606cae-d540-436f-b71e-2548387f79aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nbre of samples in validation set:  3803 \n",
            "\n",
            "here are the classes distribution :\n",
            "\n",
            "class_1 : 679\n",
            "class_2 : 1932\n",
            "class_3 : 499\n",
            "class_4 : 130\n",
            "class_5 : 394\n",
            "class_6 : 36\n",
            "class_7 : 38\n",
            "class_8 : 95\n"
          ]
        }
      ],
      "source": [
        "Valid_data_dir ='Validation'\n",
        "Valid_dataset = ImageFolder(Valid_data_dir, tt.Compose([tt.Resize((128,128)), #Resizes each image\n",
        "                                            tt.ToTensor() ])) #Converts the image to a PyTorch tensor\n",
        "\n",
        "Valid_nbre_samples=len(Valid_dataset)\n",
        "print(f\"nbre of samples in validation set:  {len(Valid_dataset)} \\n\" )\n",
        "print(\"here are the classes distribution :\\n\")\n",
        "count_class = []\n",
        "for cls in sorted(os.listdir(Valid_data_dir)):\n",
        "  print(cls, ':', len(os.listdir(Valid_data_dir + '/' + cls)))\n",
        "  count_class.append(len(os.listdir(Valid_data_dir + '/' + cls)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hCdniSCjapZ"
      },
      "source": [
        "#### compute weights for the validation set (for computing balanced accuracy later)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPz7UYV6ZG5J",
        "outputId": "89ac7e11-0d6a-4c01-b0a7-07857ce9b8ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([ 0.7001,  0.2461,  0.9527,  3.6567,  1.2065, 13.2049, 12.5099,  5.0039])\n"
          ]
        }
      ],
      "source": [
        "count_class = torch.tensor(count_class)\n",
        "weights = Valid_nbre_samples/count_class/8\n",
        "print(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ETgaHyEjoZi",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3D90XB6tjoSA"
      },
      "outputs": [],
      "source": [
        "batch_size=128\n",
        "\n",
        "train_dl = DataLoader(Train_dataset,\n",
        "                      batch_size,\n",
        "                      shuffle=True, #Randomly shuffles the data at the beginning of each epoch\n",
        "                      num_workers=4, #Number of subprocesses to use for data loading. This can speed up data loading, especially when loading from disk\n",
        "                      pin_memory=True) #Copies data to CUDA-pinned memory, which can improve the transfer speed to the GPU\n",
        "\n",
        "val_dl = DataLoader(Valid_dataset,\n",
        "                    batch_size,\n",
        "                    num_workers=4,\n",
        "                    pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rg2QFRa1g1F",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Preparing for GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4-elk7vjoQQ"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')\n",
        "\n",
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jftczhujoOB",
        "outputId": "f22b2590-8763-405f-c546-1f4da2ef296c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the device we are using is : cuda\n",
            "cuda is available\n",
            "img.device cpu\n",
            "img_gpu.devicecuda:0\n"
          ]
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "print(f\"the device we are using is : {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"cuda is available\")\n",
        "else :\n",
        "    print(\"there is no cuda\")\n",
        "img, label = Train_dataset[11]\n",
        "print(f\"img.device {img.device}\")\n",
        "img_gpu = to_device(img, device)\n",
        "\n",
        "weights= to_device(weights,device)\n",
        "print(f\"img_gpu.device{img_gpu.device}\")\n",
        "train_dl = DeviceDataLoader(train_dl, device)\n",
        "val_dl = DeviceDataLoader(val_dl, device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtdjrS4Vjxxg",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Defining the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_8S7PkkkOkL",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Defining base class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NjSmYPfMjoL4"
      },
      "outputs": [],
      "source": [
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch,weights=weights):\n",
        "        images, labels = batch\n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch\n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels,weight=weights)   # Calculate weighted cross entropy on the unbalanced validation set\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs,Valid_nbre_samples):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).sum()/Valid_nbre_samples     # Combine accuracies\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "\n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"End of Epoch, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
        "             result['train_loss'], result['val_loss'], result['val_acc']))\n",
        "\n",
        "def accuracy(outputs, labels,weights=weights):\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    w = torch.tensor([weights[i] for i in labels ])\n",
        "    w = to_device(w,device)\n",
        "    return torch.tensor(torch.sum(w*(preds == labels)).item() )\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, val_loader):\n",
        "    model.eval()\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs,batch_size*len(outputs))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.Adam):\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "\n",
        "        start_time = datetime.datetime.now()\n",
        "        for idx,batch in enumerate (train_loader):\n",
        "            loss = model.training_step(batch)\n",
        "            train_losses.append(loss)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "       # Validation phase\n",
        "\n",
        "        end_time = datetime.datetime.now()\n",
        "        execution_time = end_time - start_time\n",
        "        print(\"Execution time:\", execution_time)\n",
        "\n",
        "        result = evaluate(model, val_loader)\n",
        "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
        "        print(f\"epoch : {epoch},result : result\")\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history\n",
        "\n",
        "def conv_block(in_channels, out_channels, pool=False):\n",
        "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "              nn.BatchNorm2d(out_channels),\n",
        "              nn.ReLU(inplace=True)]\n",
        "    if pool: layers.append(nn.MaxPool2d(2))\n",
        "    return nn.Sequential(*layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhC3E3jfkcfJ",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Defining the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CioEyf3ajoJp"
      },
      "outputs": [],
      "source": [
        "class ResNet9(ImageClassificationBase):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = conv_block(in_channels, 64)\n",
        "        self.conv2 = conv_block(64, 128, pool=True)\n",
        "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
        "\n",
        "        self.conv3 = conv_block(128, 256, pool=True)\n",
        "        self.conv4 = conv_block(256, 512, pool=True)\n",
        "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
        "\n",
        "        self.classifier = nn.Sequential(nn.AdaptiveMaxPool2d(1),\n",
        "                                        nn.Flatten(),\n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.Linear(512, num_classes))\n",
        "\n",
        "    def forward(self, xb):\n",
        "        out = self.conv1(xb)\n",
        "        out = self.conv2(out)\n",
        "        out = self.res1(out) + out\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.res2(out) + out\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxuPE1ZJkjXh",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Declaring the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLYqyTInWAe6"
      },
      "outputs": [],
      "source": [
        "model = to_device(ResNet9(3, len(count_class)), device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AjsW5YMmsbG"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKxSXULMkv7S",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Uploading a previous check point if possible (ignored for the start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9NnGXzGkvwA"
      },
      "outputs": [],
      "source": [
        "filename = 'skin-classification-resnet9-128-resolution-best-valid.pth'\n",
        "# Check if the file exists\n",
        "#if os.path.exists(filename):\n",
        " #  print(f\"The file {filename} exists and now loading the params.\")\n",
        "  # model.load_state_dict(torch.load(filename))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3ohp1Gl8U4U"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRxLgE4Zkvtl",
        "outputId": "8b85572c-9ae3-4f2d-c88e-0da6b95a708c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sarting point in training\n",
            " [{'val_loss': 2.0897417068481445, 'val_acc': 0.12379557639360428}]\n"
          ]
        }
      ],
      "source": [
        "history=[]\n",
        "to_device(history,device)\n",
        "history = [evaluate(model, val_dl)]\n",
        "print(f\"sarting point in training\\n\",history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7iyqaOb8U4Y",
        "outputId": "bdad9e4f-374b-4992-a44b-06a5aaac7150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution time: 0:03:12.580724\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 1.6547, val_loss: 1.2750, val_acc: 0.3085\n",
            "Execution time: 0:03:10.216611\n",
            "epoch : 1,result : result\n",
            "End of Epoch, train_loss: 1.2805, val_loss: 1.1425, val_acc: 0.3669\n",
            "Execution time: 0:03:12.966726\n",
            "epoch : 2,result : result\n",
            "End of Epoch, train_loss: 1.1082, val_loss: 1.0981, val_acc: 0.4734\n",
            "Execution time: 0:03:14.180937\n",
            "epoch : 3,result : result\n",
            "End of Epoch, train_loss: 0.9832, val_loss: 1.0590, val_acc: 0.4024\n",
            "Execution time: 0:03:36.466099\n",
            "epoch : 4,result : result\n",
            "End of Epoch, train_loss: 0.8912, val_loss: 1.0693, val_acc: 0.4622\n",
            "Execution time: 0:03:35.779764\n",
            "epoch : 5,result : result\n",
            "End of Epoch, train_loss: 0.8169, val_loss: 0.9825, val_acc: 0.5126\n",
            "Execution time: 0:03:19.875709\n",
            "epoch : 6,result : result\n",
            "End of Epoch, train_loss: 0.7278, val_loss: 0.9832, val_acc: 0.4580\n",
            "Execution time: 0:03:18.032767\n",
            "epoch : 7,result : result\n",
            "End of Epoch, train_loss: 0.6751, val_loss: 1.0163, val_acc: 0.5311\n",
            "Execution time: 0:03:17.372301\n",
            "epoch : 8,result : result\n",
            "End of Epoch, train_loss: 0.5995, val_loss: 0.9108, val_acc: 0.5528\n",
            "Execution time: 0:03:16.491073\n",
            "epoch : 9,result : result\n",
            "End of Epoch, train_loss: 0.5453, val_loss: 0.9772, val_acc: 0.5253\n",
            "Execution time: 0:03:17.141841\n",
            "epoch : 10,result : result\n",
            "End of Epoch, train_loss: 0.4826, val_loss: 0.9330, val_acc: 0.5745\n",
            "Execution time: 0:03:15.410041\n",
            "epoch : 11,result : result\n",
            "End of Epoch, train_loss: 0.4328, val_loss: 1.0372, val_acc: 0.4655\n",
            "Execution time: 0:03:14.430214\n",
            "epoch : 12,result : result\n",
            "End of Epoch, train_loss: 0.3864, val_loss: 0.9104, val_acc: 0.5007\n",
            "Execution time: 0:03:14.253267\n",
            "epoch : 13,result : result\n",
            "End of Epoch, train_loss: 0.3475, val_loss: 0.9136, val_acc: 0.5532\n",
            "Execution time: 0:03:13.759191\n",
            "epoch : 14,result : result\n",
            "End of Epoch, train_loss: 0.3040, val_loss: 0.9183, val_acc: 0.5088\n",
            "Execution time: 0:03:15.925553\n",
            "epoch : 15,result : result\n",
            "End of Epoch, train_loss: 0.2714, val_loss: 0.8800, val_acc: 0.6151\n",
            "Execution time: 0:03:14.284095\n",
            "epoch : 16,result : result\n",
            "End of Epoch, train_loss: 0.2362, val_loss: 0.8821, val_acc: 0.6171\n",
            "Execution time: 0:03:26.987852\n",
            "epoch : 17,result : result\n",
            "End of Epoch, train_loss: 0.2111, val_loss: 0.9114, val_acc: 0.6012\n",
            "Execution time: 0:03:26.461636\n",
            "epoch : 18,result : result\n",
            "End of Epoch, train_loss: 0.1825, val_loss: 0.9421, val_acc: 0.5743\n",
            "Execution time: 0:03:26.279816\n",
            "epoch : 19,result : result\n",
            "End of Epoch, train_loss: 0.1610, val_loss: 1.0864, val_acc: 0.5458\n",
            "Execution time: 0:03:26.534439\n",
            "epoch : 20,result : result\n",
            "End of Epoch, train_loss: 0.1530, val_loss: 0.9472, val_acc: 0.6277\n",
            "Execution time: 0:03:26.799774\n",
            "epoch : 21,result : result\n",
            "End of Epoch, train_loss: 0.1392, val_loss: 0.8802, val_acc: 0.6318\n",
            "Execution time: 0:03:27.391598\n",
            "epoch : 22,result : result\n",
            "End of Epoch, train_loss: 0.1111, val_loss: 1.2998, val_acc: 0.4771\n",
            "Execution time: 0:03:28.143154\n",
            "epoch : 23,result : result\n",
            "End of Epoch, train_loss: 0.1091, val_loss: 0.9724, val_acc: 0.5629\n",
            "Execution time: 0:03:26.738943\n",
            "epoch : 24,result : result\n",
            "End of Epoch, train_loss: 0.1065, val_loss: 1.0433, val_acc: 0.5967\n",
            "Execution time: 0:03:26.975031\n",
            "epoch : 25,result : result\n",
            "End of Epoch, train_loss: 0.0946, val_loss: 0.9474, val_acc: 0.6064\n",
            "Execution time: 0:03:20.331097\n",
            "epoch : 26,result : result\n",
            "End of Epoch, train_loss: 0.0783, val_loss: 0.9937, val_acc: 0.6224\n",
            "Execution time: 0:03:24.404092\n",
            "epoch : 27,result : result\n",
            "End of Epoch, train_loss: 0.0787, val_loss: 1.0751, val_acc: 0.5992\n",
            "Execution time: 0:03:24.713411\n",
            "epoch : 28,result : result\n",
            "End of Epoch, train_loss: 0.0707, val_loss: 1.0292, val_acc: 0.5775\n",
            "Execution time: 0:03:22.863337\n",
            "epoch : 29,result : result\n",
            "End of Epoch, train_loss: 0.0727, val_loss: 0.9598, val_acc: 0.6331\n"
          ]
        }
      ],
      "source": [
        "history+=fit(30, 0.001, model, train_dl, val_dl, torch.optim.Adam)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9K76uHW8U4Z",
        "outputId": "cb65c2e8-6ee9-4627-8e9d-4818acc9a10f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch  0\n",
            "Execution time: 0:03:23.993747\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0802, val_loss: 1.0744, val_acc: 0.6404\n",
            "epoch  1\n",
            "Execution time: 0:03:24.572591\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0732, val_loss: 1.1489, val_acc: 0.5929\n",
            "epoch  2\n",
            "Execution time: 0:03:24.336067\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0637, val_loss: 1.1294, val_acc: 0.5711\n",
            "epoch  3\n",
            "Execution time: 0:03:23.171282\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0597, val_loss: 1.1074, val_acc: 0.6505\n",
            "epoch  4\n",
            "Execution time: 0:03:23.126412\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0568, val_loss: 1.0566, val_acc: 0.6592\n",
            "epoch  5\n",
            "Execution time: 0:03:23.985932\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0566, val_loss: 1.5737, val_acc: 0.4263\n",
            "epoch  6\n",
            "Execution time: 0:03:24.330363\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0581, val_loss: 1.1957, val_acc: 0.5969\n",
            "epoch  7\n",
            "Execution time: 0:03:20.807178\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0537, val_loss: 1.1674, val_acc: 0.6403\n",
            "epoch  8\n",
            "Execution time: 0:03:24.168087\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0534, val_loss: 1.2314, val_acc: 0.5772\n",
            "epoch  9\n",
            "Execution time: 0:03:24.348377\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0521, val_loss: 1.2450, val_acc: 0.6201\n",
            "epoch  10\n",
            "Execution time: 0:03:26.019574\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0472, val_loss: 1.2060, val_acc: 0.6534\n",
            "epoch  11\n",
            "Execution time: 0:03:26.222602\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0470, val_loss: 1.2321, val_acc: 0.6024\n",
            "epoch  12\n",
            "Execution time: 0:03:24.062089\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0423, val_loss: 1.2439, val_acc: 0.6125\n",
            "epoch  13\n",
            "Execution time: 0:03:25.109743\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0432, val_loss: 1.3106, val_acc: 0.5883\n",
            "epoch  14\n",
            "Execution time: 0:03:24.716427\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0404, val_loss: 1.2968, val_acc: 0.6371\n",
            "epoch  15\n",
            "Execution time: 0:03:25.297341\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0427, val_loss: 1.2766, val_acc: 0.6410\n",
            "epoch  16\n",
            "Execution time: 0:03:27.111487\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0373, val_loss: 1.4162, val_acc: 0.6437\n",
            "epoch  17\n",
            "Execution time: 0:03:21.098191\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0343, val_loss: 1.3006, val_acc: 0.6457\n",
            "epoch  18\n",
            "Execution time: 0:03:23.042489\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0356, val_loss: 1.4004, val_acc: 0.5959\n",
            "epoch  19\n",
            "Execution time: 0:03:20.305734\n",
            "epoch : 0,result : result\n",
            "End of Epoch, train_loss: 0.0348, val_loss: 1.3534, val_acc: 0.5865\n"
          ]
        }
      ],
      "source": [
        "best_acc = history[0][\"val_acc\"]\n",
        "nbr_epochs = 20\n",
        "for i in range(nbr_epochs):\n",
        "    print(\"epoch \",i)\n",
        "    history+=fit(1, 0.001, model, train_dl, val_dl, torch.optim.Adam)\n",
        "    if (history[-1][\"val_acc\"]> best_acc):\n",
        "        best_acc =history[-1][\"val_acc\"]\n",
        "        torch.save(model.state_dict(), 'skin-classification-resnet9-128-resolution-best-valid.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TFJJ-ztkvow",
        "outputId": "20a1c5cd-cba8-4bc5-86d5-252756c55ca6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'val_loss': 1.3533598184585571, 'val_acc': 0.5865016579627991}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(model, val_dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHly-rMjlPAl",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "### Saving last model weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rauf45UAkvmY"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'skin-classification-resnet9-128-resolution-last.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "pqufskQf8U4h"
      },
      "source": [
        "## Uploading the best model so far"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1HBcVYA8U4i",
        "outputId": "e52f103d-91e2-4516-a982-0a094ce6f2a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The file skin-classification-resnet9-128-resolution-best-valid.pth exists and now loading the params.\n"
          ]
        }
      ],
      "source": [
        "filename = 'skin-classification-resnet9-128-resolution-best-valid.pth'\n",
        "# Check if the file exists\n",
        "if os.path.exists(filename):\n",
        "   print(f\"The file {filename} exists and now loading the params.\")\n",
        "   model.load_state_dict(torch.load(filename))\n",
        "else :\n",
        "    print(\"Some thing went wrong, the file isn't here\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQyjUJkDlUX5",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Generating output for submission without meta data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnbA6t1zlnRD"
      },
      "outputs": [],
      "source": [
        "test_data_dir = \"./Test\"\n",
        "\n",
        "test_dataset = ImageFolder(root=test_data_dir, transform=tt.Compose([tt.Resize((128,128)), #Resizes each image\n",
        "                                            tt.ToTensor() ])) #Converts the image to a PyTorch tensor\n",
        "\n",
        "test_dl=DataLoader(test_dataset,\n",
        "                      batch_size,\n",
        "                      shuffle=False,\n",
        "                      num_workers=4,\n",
        "                      pin_memory=True)\n",
        "test_dl=DeviceDataLoader(test_dl, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acSwUxZBWAPG"
      },
      "outputs": [],
      "source": [
        "# Function to generate predictions for the test dataset\n",
        "def generate_predictions(model, test_loader):\n",
        "    predictions = []\n",
        "    for images, _ in test_loader:\n",
        "        # Forward pass to get predictions\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        predictions.extend(predicted.tolist())\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Generate predictions for the test dataset\n",
        "predicted_classes = generate_predictions(model, test_dl)\n",
        "\n",
        "# Create a CSV file and write predictions to it\n",
        "csv_file = 'predictions_without_meta_data.csv'\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['ID', 'CLASS'])  # Write header\n",
        "    for i, (image_name, _) in enumerate(test_dl.dl.dataset.samples):\n",
        "        image_name=image_name[-16:-4]\n",
        "        if (image_name[0] == 'I'):\n",
        "            writer.writerow([image_name, predicted_classes[i]+1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-dtpH9ntYxj",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Generating last layer ouput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zRsGqRxth7i"
      },
      "outputs": [],
      "source": [
        "def generate_feature_out(model, test_loader):\n",
        "    predictions = []\n",
        "    for images, _ in test_loader:\n",
        "        # Forward pass to get predictions\n",
        "\n",
        "        out = model.conv1(images)\n",
        "        out = model.conv2(out)\n",
        "        out = model.res1(out) + out\n",
        "        out = model.conv3(out)\n",
        "        out = model.conv4(out)\n",
        "        out = model.res2(out) + out\n",
        "\n",
        "        outputs = nn.Flatten()(nn.AdaptiveMaxPool2d(1)(out))\n",
        "\n",
        "        predictions.extend(outputs.tolist())\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouF0TPQOtc2d"
      },
      "outputs": [],
      "source": [
        "batch_size= 128\n",
        "Train_dataset = ImageFolder(root=Train_data_dir, transform=tt.Compose([tt.Resize((128,128)),\n",
        "                                            tt.ToTensor() ]))\n",
        "\n",
        "Train_dl=DataLoader(Train_dataset,\n",
        "                      batch_size,\n",
        "                      shuffle=False,\n",
        "                      num_workers=4,\n",
        "                      pin_memory=True)\n",
        "Train_dl=DeviceDataLoader(Train_dl, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqI8Uflgtczk"
      },
      "outputs": [],
      "source": [
        "Valid_dataset = ImageFolder(Valid_data_dir, transform=tt.Compose([tt.Resize((128,128)),\n",
        "                                            tt.ToTensor() ]))\n",
        "Valid_dl=DataLoader(Valid_dataset,\n",
        "                      batch_size,\n",
        "                      shuffle=False,\n",
        "                      num_workers=4,\n",
        "                      pin_memory=True)\n",
        "Valid_dl=DeviceDataLoader(Valid_dl, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiMhm2Twvxyz"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the test dataset\n",
        "predicted_classes = generate_feature_out(model, Train_dl)\n",
        "\n",
        "# Create a CSV file and write predictions to it\n",
        "csv_file = 'features_out_train_ResNet9_128size.csv'\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['ID']+list(range(512)))  # Write header\n",
        "    for i, (image_name, _) in enumerate(Train_dl.dl.dataset.samples):\n",
        "        image_name=image_name[-16:-4]\n",
        "        if (image_name[0] == 'I'):\n",
        "            writer.writerow([image_name]+ predicted_classes[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zadKBcYcth2q"
      },
      "outputs": [],
      "source": [
        "predicted_classes = generate_feature_out(model, Valid_dl)\n",
        "\n",
        "# Create a CSV file and write predictions to it\n",
        "csv_file = 'features_out_valid_ResNet9_128size.csv'\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['ID']+list(range(512)))  # Write header\n",
        "    for i, (image_name, _) in enumerate(Valid_dl.dl.dataset.samples):\n",
        "        image_name=image_name[-16:-4]\n",
        "        if (image_name[0] == 'I'):\n",
        "            writer.writerow([image_name]+ predicted_classes[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oq9_Q5_U8U4m"
      },
      "outputs": [],
      "source": [
        "del Valid_dataset\n",
        "del Train_dataset\n",
        "del train_dl\n",
        "del Valid_dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE-A9YMM8U4n"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n1fT2hH8U4n"
      },
      "outputs": [],
      "source": [
        "test_data_dir = \"./Test\"\n",
        "\n",
        "test_dataset = ImageFolder(root=test_data_dir, transform=tt.Compose([tt.Resize((128,128)),\n",
        "                                            tt.ToTensor() ]))\n",
        "\n",
        "test_dl=DataLoader(test_dataset,\n",
        "                      batch_size,\n",
        "                      shuffle=False,\n",
        "                      num_workers=4,\n",
        "                      pin_memory=True)\n",
        "test_dl=DeviceDataLoader(test_dl, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqDhVZivto1E"
      },
      "outputs": [],
      "source": [
        "# Generate predictions for the test dataset\n",
        "predicted_classes = generate_feature_out(model, test_dl)\n",
        "\n",
        "# Create a CSV file and write predictions to it\n",
        "csv_file = 'features_out_test_ResNet9_acc_128size.csv'\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['ID']+list(range(512)))  # Write header\n",
        "    for i, (image_name, _) in enumerate(test_dl.dl.dataset.samples):\n",
        "        image_name=image_name[-16:-4]\n",
        "        writer.writerow([image_name]+ predicted_classes[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4kg9-LEm5bo",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Working with metaData"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "5Bl702tp8U4q"
      },
      "source": [
        "## Reading and merging the different features of the images in a data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jHC2y6Dm4zK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#  Read the first CSV file\n",
        "file1 = pd.read_csv('metadataTrain.csv')\n",
        "\n",
        "# Read the second CSV file\n",
        "file2 = pd.read_csv('features_out_train_ResNet9_128size.csv')\n",
        "file3 = pd.read_csv('features_out_valid_ResNet9_128size.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UnyXZSrwdAQ",
        "outputId": "e809ba94-d034-4833-bde6-e5653e2e8f6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['ID', '0', '1', '2', '3', '4', '5', '6', '7', '8',\n",
              "       ...\n",
              "       '502', '503', '504', '505', '506', '507', '508', '509', '510', '511'],\n",
              "      dtype='object', length=513)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file2.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUekZyJQwfFc"
      },
      "outputs": [],
      "source": [
        "# Merge the two DataFrames on the 'id' column\n",
        "df = pd.merge(file1, file2, on='ID').sort_values(by='ID')\n",
        "\n",
        "df_valid = pd.merge(file1, file3, on='ID').sort_values(by='ID')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYcKAohXwvZX",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Dealing with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "id": "olXaTMQTwfDU",
        "outputId": "3189031e-a06a-42fc-c348-e88a14822548"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>CLASS</th>\n",
              "      <th>SEX</th>\n",
              "      <th>AGE</th>\n",
              "      <th>POSITION</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>...</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6443</th>\n",
              "      <td>ISIC_0000000</td>\n",
              "      <td>2</td>\n",
              "      <td>female</td>\n",
              "      <td>55.0</td>\n",
              "      <td>anterior torso</td>\n",
              "      <td>1.150458</td>\n",
              "      <td>2.980487</td>\n",
              "      <td>1.506404</td>\n",
              "      <td>1.217551</td>\n",
              "      <td>2.277071</td>\n",
              "      <td>...</td>\n",
              "      <td>4.442955</td>\n",
              "      <td>2.429837</td>\n",
              "      <td>1.004668</td>\n",
              "      <td>1.056447</td>\n",
              "      <td>1.268132</td>\n",
              "      <td>0.067959</td>\n",
              "      <td>0.529625</td>\n",
              "      <td>1.121799</td>\n",
              "      <td>2.559214</td>\n",
              "      <td>0.376304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8363</th>\n",
              "      <td>ISIC_0000001</td>\n",
              "      <td>2</td>\n",
              "      <td>female</td>\n",
              "      <td>30.0</td>\n",
              "      <td>anterior torso</td>\n",
              "      <td>0.491326</td>\n",
              "      <td>3.270404</td>\n",
              "      <td>3.081271</td>\n",
              "      <td>0.461111</td>\n",
              "      <td>1.465060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.054244</td>\n",
              "      <td>1.555666</td>\n",
              "      <td>0.969552</td>\n",
              "      <td>0.613168</td>\n",
              "      <td>0.559218</td>\n",
              "      <td>1.911094</td>\n",
              "      <td>0.611309</td>\n",
              "      <td>3.330431</td>\n",
              "      <td>2.685349</td>\n",
              "      <td>1.079589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10794</th>\n",
              "      <td>ISIC_0000002</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>60.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>2.231386</td>\n",
              "      <td>4.724333</td>\n",
              "      <td>2.979819</td>\n",
              "      <td>0.815685</td>\n",
              "      <td>1.704069</td>\n",
              "      <td>...</td>\n",
              "      <td>3.598701</td>\n",
              "      <td>3.590002</td>\n",
              "      <td>1.647864</td>\n",
              "      <td>1.230797</td>\n",
              "      <td>3.275215</td>\n",
              "      <td>0.403538</td>\n",
              "      <td>1.276834</td>\n",
              "      <td>3.084767</td>\n",
              "      <td>2.304701</td>\n",
              "      <td>1.673227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4216</th>\n",
              "      <td>ISIC_0000003</td>\n",
              "      <td>2</td>\n",
              "      <td>male</td>\n",
              "      <td>30.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>1.931306</td>\n",
              "      <td>4.549762</td>\n",
              "      <td>2.341205</td>\n",
              "      <td>0.483503</td>\n",
              "      <td>4.756865</td>\n",
              "      <td>...</td>\n",
              "      <td>1.405564</td>\n",
              "      <td>2.046002</td>\n",
              "      <td>1.832454</td>\n",
              "      <td>1.544143</td>\n",
              "      <td>1.042329</td>\n",
              "      <td>1.719378</td>\n",
              "      <td>0.741069</td>\n",
              "      <td>2.670605</td>\n",
              "      <td>1.456814</td>\n",
              "      <td>0.928056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4731</th>\n",
              "      <td>ISIC_0000004</td>\n",
              "      <td>1</td>\n",
              "      <td>male</td>\n",
              "      <td>80.0</td>\n",
              "      <td>posterior torso</td>\n",
              "      <td>2.467550</td>\n",
              "      <td>2.135215</td>\n",
              "      <td>1.103954</td>\n",
              "      <td>3.704881</td>\n",
              "      <td>1.759375</td>\n",
              "      <td>...</td>\n",
              "      <td>2.195263</td>\n",
              "      <td>2.097261</td>\n",
              "      <td>4.026792</td>\n",
              "      <td>1.339238</td>\n",
              "      <td>2.045606</td>\n",
              "      <td>1.580635</td>\n",
              "      <td>4.386715</td>\n",
              "      <td>1.143096</td>\n",
              "      <td>6.871482</td>\n",
              "      <td>0.561664</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8487</th>\n",
              "      <td>ISIC_0073229</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>55.0</td>\n",
              "      <td>anterior torso</td>\n",
              "      <td>0.301811</td>\n",
              "      <td>0.487591</td>\n",
              "      <td>0.467382</td>\n",
              "      <td>2.079736</td>\n",
              "      <td>2.726546</td>\n",
              "      <td>...</td>\n",
              "      <td>0.053469</td>\n",
              "      <td>0.908656</td>\n",
              "      <td>1.300288</td>\n",
              "      <td>0.708439</td>\n",
              "      <td>0.598253</td>\n",
              "      <td>1.224248</td>\n",
              "      <td>0.542073</td>\n",
              "      <td>0.531251</td>\n",
              "      <td>1.102682</td>\n",
              "      <td>1.794416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3685</th>\n",
              "      <td>ISIC_0073238</td>\n",
              "      <td>1</td>\n",
              "      <td>female</td>\n",
              "      <td>45.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>0.280050</td>\n",
              "      <td>2.088145</td>\n",
              "      <td>0.863014</td>\n",
              "      <td>0.762013</td>\n",
              "      <td>1.402689</td>\n",
              "      <td>...</td>\n",
              "      <td>0.259937</td>\n",
              "      <td>1.750433</td>\n",
              "      <td>1.694401</td>\n",
              "      <td>0.910482</td>\n",
              "      <td>0.971135</td>\n",
              "      <td>1.009460</td>\n",
              "      <td>0.962540</td>\n",
              "      <td>0.815566</td>\n",
              "      <td>1.588802</td>\n",
              "      <td>1.686631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13598</th>\n",
              "      <td>ISIC_0073244</td>\n",
              "      <td>2</td>\n",
              "      <td>male</td>\n",
              "      <td>15.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>1.685501</td>\n",
              "      <td>2.864002</td>\n",
              "      <td>1.139722</td>\n",
              "      <td>1.487474</td>\n",
              "      <td>1.556488</td>\n",
              "      <td>...</td>\n",
              "      <td>0.432138</td>\n",
              "      <td>2.172874</td>\n",
              "      <td>1.887555</td>\n",
              "      <td>2.504567</td>\n",
              "      <td>1.063905</td>\n",
              "      <td>1.763023</td>\n",
              "      <td>2.204026</td>\n",
              "      <td>2.518091</td>\n",
              "      <td>2.302393</td>\n",
              "      <td>1.392523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2106</th>\n",
              "      <td>ISIC_0073245</td>\n",
              "      <td>2</td>\n",
              "      <td>female</td>\n",
              "      <td>45.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>0.606092</td>\n",
              "      <td>3.654668</td>\n",
              "      <td>1.106127</td>\n",
              "      <td>1.445089</td>\n",
              "      <td>1.663880</td>\n",
              "      <td>...</td>\n",
              "      <td>0.376693</td>\n",
              "      <td>2.294310</td>\n",
              "      <td>3.702431</td>\n",
              "      <td>1.527394</td>\n",
              "      <td>0.859017</td>\n",
              "      <td>2.075352</td>\n",
              "      <td>2.495228</td>\n",
              "      <td>2.648034</td>\n",
              "      <td>1.277629</td>\n",
              "      <td>0.806385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3019</th>\n",
              "      <td>ISIC_0073251</td>\n",
              "      <td>2</td>\n",
              "      <td>female</td>\n",
              "      <td>55.0</td>\n",
              "      <td>palms/soles</td>\n",
              "      <td>0.490116</td>\n",
              "      <td>0.842758</td>\n",
              "      <td>1.807798</td>\n",
              "      <td>0.925831</td>\n",
              "      <td>2.129036</td>\n",
              "      <td>...</td>\n",
              "      <td>0.152492</td>\n",
              "      <td>0.571923</td>\n",
              "      <td>0.827529</td>\n",
              "      <td>0.756758</td>\n",
              "      <td>0.778139</td>\n",
              "      <td>2.003504</td>\n",
              "      <td>1.604620</td>\n",
              "      <td>1.407818</td>\n",
              "      <td>0.407415</td>\n",
              "      <td>0.558991</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15195 rows × 517 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                 ID  CLASS     SEX   AGE         POSITION         0         1  \\\n",
              "6443   ISIC_0000000      2  female  55.0   anterior torso  1.150458  2.980487   \n",
              "8363   ISIC_0000001      2  female  30.0   anterior torso  0.491326  3.270404   \n",
              "10794  ISIC_0000002      1  female  60.0  upper extremity  2.231386  4.724333   \n",
              "4216   ISIC_0000003      2    male  30.0  upper extremity  1.931306  4.549762   \n",
              "4731   ISIC_0000004      1    male  80.0  posterior torso  2.467550  2.135215   \n",
              "...             ...    ...     ...   ...              ...       ...       ...   \n",
              "8487   ISIC_0073229      3    male  55.0   anterior torso  0.301811  0.487591   \n",
              "3685   ISIC_0073238      1  female  45.0  upper extremity  0.280050  2.088145   \n",
              "13598  ISIC_0073244      2    male  15.0  lower extremity  1.685501  2.864002   \n",
              "2106   ISIC_0073245      2  female  45.0  upper extremity  0.606092  3.654668   \n",
              "3019   ISIC_0073251      2  female  55.0      palms/soles  0.490116  0.842758   \n",
              "\n",
              "              2         3         4  ...       502       503       504  \\\n",
              "6443   1.506404  1.217551  2.277071  ...  4.442955  2.429837  1.004668   \n",
              "8363   3.081271  0.461111  1.465060  ...  0.054244  1.555666  0.969552   \n",
              "10794  2.979819  0.815685  1.704069  ...  3.598701  3.590002  1.647864   \n",
              "4216   2.341205  0.483503  4.756865  ...  1.405564  2.046002  1.832454   \n",
              "4731   1.103954  3.704881  1.759375  ...  2.195263  2.097261  4.026792   \n",
              "...         ...       ...       ...  ...       ...       ...       ...   \n",
              "8487   0.467382  2.079736  2.726546  ...  0.053469  0.908656  1.300288   \n",
              "3685   0.863014  0.762013  1.402689  ...  0.259937  1.750433  1.694401   \n",
              "13598  1.139722  1.487474  1.556488  ...  0.432138  2.172874  1.887555   \n",
              "2106   1.106127  1.445089  1.663880  ...  0.376693  2.294310  3.702431   \n",
              "3019   1.807798  0.925831  2.129036  ...  0.152492  0.571923  0.827529   \n",
              "\n",
              "            505       506       507       508       509       510       511  \n",
              "6443   1.056447  1.268132  0.067959  0.529625  1.121799  2.559214  0.376304  \n",
              "8363   0.613168  0.559218  1.911094  0.611309  3.330431  2.685349  1.079589  \n",
              "10794  1.230797  3.275215  0.403538  1.276834  3.084767  2.304701  1.673227  \n",
              "4216   1.544143  1.042329  1.719378  0.741069  2.670605  1.456814  0.928056  \n",
              "4731   1.339238  2.045606  1.580635  4.386715  1.143096  6.871482  0.561664  \n",
              "...         ...       ...       ...       ...       ...       ...       ...  \n",
              "8487   0.708439  0.598253  1.224248  0.542073  0.531251  1.102682  1.794416  \n",
              "3685   0.910482  0.971135  1.009460  0.962540  0.815566  1.588802  1.686631  \n",
              "13598  2.504567  1.063905  1.763023  2.204026  2.518091  2.302393  1.392523  \n",
              "2106   1.527394  0.859017  2.075352  2.495228  2.648034  1.277629  0.806385  \n",
              "3019   0.756758  0.778139  2.003504  1.604620  1.407818  0.407415  0.558991  \n",
              "\n",
              "[15195 rows x 517 columns]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['SEX'].fillna('male', inplace=True)\n",
        "df_valid['SEX'].fillna('male', inplace=True)\n",
        "fill_sex_for_test = 'male'\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZMNFN25wfA7",
        "outputId": "8aef0194-786f-4278-ce34-fad1b25ba3af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CLASS\n",
              "1    60.0\n",
              "2    45.0\n",
              "3    70.0\n",
              "4    70.0\n",
              "5    65.0\n",
              "6    50.0\n",
              "7    55.0\n",
              "8    75.0\n",
              "Name: AGE, dtype: float64"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate median age for each class\n",
        "median_age_by_class = df.groupby('CLASS')['AGE'].median()\n",
        "median_age_by_class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imdGeuzDwe-y"
      },
      "outputs": [],
      "source": [
        "# Fill missing values in the 'age' column with the corresponding median age for each class\n",
        "for class_value, median_age in median_age_by_class.items():\n",
        "    df.loc[df['CLASS'] == class_value, 'AGE'] = df.loc[df['CLASS'] == class_value, 'AGE'].fillna(median_age)\n",
        "    df_valid.loc[df_valid['CLASS'] == class_value, 'AGE'] = df_valid.loc[df_valid['CLASS'] == class_value, 'AGE'].fillna(median_age)\n",
        "fill_age_for_test = median_age_by_class.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMpFSV0uwe8e",
        "outputId": "f1d35d2a-6819-49c8-9529-1ca8253c7fbd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CLASS\n",
              "1     anterior torso\n",
              "2     anterior torso\n",
              "3          head/neck\n",
              "4          head/neck\n",
              "5          head/neck\n",
              "6    lower extremity\n",
              "7     anterior torso\n",
              "8    lower extremity\n",
              "Name: POSITION, dtype: object"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate the most frequent position for each class\n",
        "most_frequent_position_by_class = df.groupby('CLASS')['POSITION'].agg(lambda x: x.mode().iloc[0])\n",
        "most_frequent_position_by_class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eb2V5bf4w1b4"
      },
      "outputs": [],
      "source": [
        "df['POSITION'].fillna('anterior torso', inplace=True)\n",
        "df_valid['POSITION'].fillna('anterior torso', inplace=True)\n",
        "fill_position_for_test = 'anterior torso'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zc1WkAk6w1Z2"
      },
      "outputs": [],
      "source": [
        "# Define the bins and labels for age categories\n",
        "bins = [ 0.,  5., 10., 15., 20., 25., 30., 35., 40., 45., 50., 55., 60.,\n",
        "       65., 70., 75., 80., 85., float('inf')]\n",
        "labels = list(range(len(bins)-1))\n",
        "\n",
        "# Create a new column 'age_category' by binning the 'age' column\n",
        "df['age_category'] = pd.cut(df['AGE'], bins=bins, labels=labels, include_lowest=True)\n",
        "df_valid['age_category'] = pd.cut(df_valid['AGE'], bins=bins, labels=labels, include_lowest=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff6H1PZew1Xg",
        "outputId": "4e2a7a0c-91c9-4899-dcdd-9879c31a08eb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['CLASS', 'SEX', 'POSITION', '0', '1', '2', '3', '4', '5', '6',\n",
              "       ...\n",
              "       '503', '504', '505', '506', '507', '508', '509', '510', '511',\n",
              "       'age_category'],\n",
              "      dtype='object', length=516)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove the 'ID' and 'age' columns\n",
        "df.drop(['ID', 'AGE'], axis=1, inplace=True)\n",
        "df_valid.drop(['ID', 'AGE'], axis=1, inplace=True)\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8R4K7iiYw1VL",
        "outputId": "3520a4e9-3cf6-475a-900c-cea0089d163c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features (X):\n",
            "Index(['SEX', 'POSITION', '0', '1', '2', '3', '4', '5', '6', '7',\n",
            "       ...\n",
            "       '503', '504', '505', '506', '507', '508', '509', '510', '511',\n",
            "       'age_category'],\n",
            "      dtype='object', length=515)\n"
          ]
        }
      ],
      "source": [
        "# Separate features (X) and labels (y)\n",
        "y = df['CLASS']  # Select only the 'class' column as the label\n",
        "X = df.drop('CLASS', axis=1)  # Drop the 'class' column to keep only the features\n",
        "\n",
        "y_valid = df_valid['CLASS']  # Select only the 'class' column as the label\n",
        "X_valid = df_valid.drop('CLASS', axis=1)  # Drop the 'class' column to keep only the features\n",
        "# Display the features and labels DataFrames\n",
        "print(\"Features (X):\")\n",
        "print(X.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "qFr-w4hG8U46"
      },
      "source": [
        "## Reading test features in a dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvOXnKbs8U47"
      },
      "outputs": [],
      "source": [
        "#  Read the first CSV file\n",
        "file1 = pd.read_csv('metadataTest.csv')\n",
        "\n",
        "# Read the second CSV file\n",
        "file2 = pd.read_csv('features_out_test_ResNet9_acc_128size.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bet0LR-L8U4-",
        "outputId": "f56d4582-b18e-4a03-80fd-a40f5c448973"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>SEX</th>\n",
              "      <th>AGE</th>\n",
              "      <th>POSITION</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>...</th>\n",
              "      <th>502</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>ISIC_0000006</td>\n",
              "      <td>female</td>\n",
              "      <td>25.0</td>\n",
              "      <td>posterior torso</td>\n",
              "      <td>0.833705</td>\n",
              "      <td>1.060439</td>\n",
              "      <td>1.023758</td>\n",
              "      <td>0.967986</td>\n",
              "      <td>1.432524</td>\n",
              "      <td>0.081379</td>\n",
              "      <td>...</td>\n",
              "      <td>1.295570</td>\n",
              "      <td>2.054084</td>\n",
              "      <td>1.114262</td>\n",
              "      <td>0.779856</td>\n",
              "      <td>1.652471</td>\n",
              "      <td>0.326921</td>\n",
              "      <td>1.260001</td>\n",
              "      <td>1.834785</td>\n",
              "      <td>0.982471</td>\n",
              "      <td>0.643780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4191</th>\n",
              "      <td>ISIC_0000011</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>0.520693</td>\n",
              "      <td>1.775505</td>\n",
              "      <td>1.454161</td>\n",
              "      <td>0.384557</td>\n",
              "      <td>2.993728</td>\n",
              "      <td>0.438422</td>\n",
              "      <td>...</td>\n",
              "      <td>0.140466</td>\n",
              "      <td>1.722538</td>\n",
              "      <td>1.326782</td>\n",
              "      <td>0.951317</td>\n",
              "      <td>0.767415</td>\n",
              "      <td>0.734094</td>\n",
              "      <td>0.297562</td>\n",
              "      <td>4.567427</td>\n",
              "      <td>4.015381</td>\n",
              "      <td>1.085661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>ISIC_0000014</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>posterior torso</td>\n",
              "      <td>0.410760</td>\n",
              "      <td>1.611643</td>\n",
              "      <td>0.729729</td>\n",
              "      <td>0.279183</td>\n",
              "      <td>1.355647</td>\n",
              "      <td>0.337431</td>\n",
              "      <td>...</td>\n",
              "      <td>0.271497</td>\n",
              "      <td>1.830468</td>\n",
              "      <td>0.873747</td>\n",
              "      <td>0.523680</td>\n",
              "      <td>0.579251</td>\n",
              "      <td>0.763787</td>\n",
              "      <td>0.231753</td>\n",
              "      <td>2.854530</td>\n",
              "      <td>0.784511</td>\n",
              "      <td>1.215255</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4989</th>\n",
              "      <td>ISIC_0000018</td>\n",
              "      <td>male</td>\n",
              "      <td>30.0</td>\n",
              "      <td>posterior torso</td>\n",
              "      <td>1.127246</td>\n",
              "      <td>1.793517</td>\n",
              "      <td>0.763771</td>\n",
              "      <td>0.345557</td>\n",
              "      <td>1.504942</td>\n",
              "      <td>0.274724</td>\n",
              "      <td>...</td>\n",
              "      <td>0.024787</td>\n",
              "      <td>2.109586</td>\n",
              "      <td>2.934217</td>\n",
              "      <td>2.101435</td>\n",
              "      <td>0.662123</td>\n",
              "      <td>0.779637</td>\n",
              "      <td>0.329928</td>\n",
              "      <td>2.556285</td>\n",
              "      <td>2.060014</td>\n",
              "      <td>0.602126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4845</th>\n",
              "      <td>ISIC_0000022</td>\n",
              "      <td>female</td>\n",
              "      <td>55.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>0.315831</td>\n",
              "      <td>2.786491</td>\n",
              "      <td>1.038399</td>\n",
              "      <td>0.786178</td>\n",
              "      <td>1.367861</td>\n",
              "      <td>0.946043</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.975137</td>\n",
              "      <td>2.772453</td>\n",
              "      <td>1.588256</td>\n",
              "      <td>0.553294</td>\n",
              "      <td>1.165043</td>\n",
              "      <td>0.928831</td>\n",
              "      <td>3.515904</td>\n",
              "      <td>1.497891</td>\n",
              "      <td>1.511364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4253</th>\n",
              "      <td>ISIC_0073240</td>\n",
              "      <td>female</td>\n",
              "      <td>45.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>1.006920</td>\n",
              "      <td>4.349065</td>\n",
              "      <td>2.122132</td>\n",
              "      <td>0.478539</td>\n",
              "      <td>3.475388</td>\n",
              "      <td>0.302254</td>\n",
              "      <td>...</td>\n",
              "      <td>0.259459</td>\n",
              "      <td>1.956834</td>\n",
              "      <td>1.301193</td>\n",
              "      <td>5.171215</td>\n",
              "      <td>0.700102</td>\n",
              "      <td>1.314510</td>\n",
              "      <td>0.467656</td>\n",
              "      <td>3.209450</td>\n",
              "      <td>4.060209</td>\n",
              "      <td>0.656931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2085</th>\n",
              "      <td>ISIC_0073246</td>\n",
              "      <td>male</td>\n",
              "      <td>80.0</td>\n",
              "      <td>anterior torso</td>\n",
              "      <td>0.589952</td>\n",
              "      <td>1.750752</td>\n",
              "      <td>2.006328</td>\n",
              "      <td>3.812641</td>\n",
              "      <td>2.159756</td>\n",
              "      <td>0.246238</td>\n",
              "      <td>...</td>\n",
              "      <td>0.593444</td>\n",
              "      <td>1.551995</td>\n",
              "      <td>1.452943</td>\n",
              "      <td>2.396023</td>\n",
              "      <td>2.059243</td>\n",
              "      <td>1.407080</td>\n",
              "      <td>0.591397</td>\n",
              "      <td>1.037485</td>\n",
              "      <td>3.657440</td>\n",
              "      <td>1.441381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3900</th>\n",
              "      <td>ISIC_0073247</td>\n",
              "      <td>female</td>\n",
              "      <td>85.0</td>\n",
              "      <td>head/neck</td>\n",
              "      <td>1.196528</td>\n",
              "      <td>0.874397</td>\n",
              "      <td>0.807927</td>\n",
              "      <td>0.757049</td>\n",
              "      <td>3.170386</td>\n",
              "      <td>1.124121</td>\n",
              "      <td>...</td>\n",
              "      <td>1.489706</td>\n",
              "      <td>2.728544</td>\n",
              "      <td>3.334295</td>\n",
              "      <td>1.660389</td>\n",
              "      <td>0.344655</td>\n",
              "      <td>1.022761</td>\n",
              "      <td>1.860433</td>\n",
              "      <td>1.060542</td>\n",
              "      <td>1.577378</td>\n",
              "      <td>2.764105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4666</th>\n",
              "      <td>ISIC_0073249</td>\n",
              "      <td>male</td>\n",
              "      <td>70.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>2.500076</td>\n",
              "      <td>3.883769</td>\n",
              "      <td>2.990919</td>\n",
              "      <td>2.438957</td>\n",
              "      <td>3.287937</td>\n",
              "      <td>1.642361</td>\n",
              "      <td>...</td>\n",
              "      <td>1.565336</td>\n",
              "      <td>1.510220</td>\n",
              "      <td>2.828382</td>\n",
              "      <td>1.659220</td>\n",
              "      <td>0.788920</td>\n",
              "      <td>2.494278</td>\n",
              "      <td>1.477300</td>\n",
              "      <td>2.348673</td>\n",
              "      <td>3.727552</td>\n",
              "      <td>3.094683</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>ISIC_0073254</td>\n",
              "      <td>male</td>\n",
              "      <td>50.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>0.787703</td>\n",
              "      <td>2.927067</td>\n",
              "      <td>1.436209</td>\n",
              "      <td>0.473223</td>\n",
              "      <td>3.572704</td>\n",
              "      <td>0.717079</td>\n",
              "      <td>...</td>\n",
              "      <td>0.266683</td>\n",
              "      <td>2.033299</td>\n",
              "      <td>2.407977</td>\n",
              "      <td>0.912128</td>\n",
              "      <td>0.864750</td>\n",
              "      <td>1.138418</td>\n",
              "      <td>1.234299</td>\n",
              "      <td>3.569176</td>\n",
              "      <td>1.418533</td>\n",
              "      <td>1.386886</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6333 rows × 516 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                ID     SEX   AGE         POSITION         0         1  \\\n",
              "338   ISIC_0000006  female  25.0  posterior torso  0.833705  1.060439   \n",
              "4191  ISIC_0000011  female  35.0  lower extremity  0.520693  1.775505   \n",
              "2000  ISIC_0000014    male  35.0  posterior torso  0.410760  1.611643   \n",
              "4989  ISIC_0000018    male  30.0  posterior torso  1.127246  1.793517   \n",
              "4845  ISIC_0000022  female  55.0  lower extremity  0.315831  2.786491   \n",
              "...            ...     ...   ...              ...       ...       ...   \n",
              "4253  ISIC_0073240  female  45.0  lower extremity  1.006920  4.349065   \n",
              "2085  ISIC_0073246    male  80.0   anterior torso  0.589952  1.750752   \n",
              "3900  ISIC_0073247  female  85.0        head/neck  1.196528  0.874397   \n",
              "4666  ISIC_0073249    male  70.0  lower extremity  2.500076  3.883769   \n",
              "460   ISIC_0073254    male  50.0  upper extremity  0.787703  2.927067   \n",
              "\n",
              "             2         3         4         5  ...       502       503  \\\n",
              "338   1.023758  0.967986  1.432524  0.081379  ...  1.295570  2.054084   \n",
              "4191  1.454161  0.384557  2.993728  0.438422  ...  0.140466  1.722538   \n",
              "2000  0.729729  0.279183  1.355647  0.337431  ...  0.271497  1.830468   \n",
              "4989  0.763771  0.345557  1.504942  0.274724  ...  0.024787  2.109586   \n",
              "4845  1.038399  0.786178  1.367861  0.946043  ...  0.000000  2.975137   \n",
              "...        ...       ...       ...       ...  ...       ...       ...   \n",
              "4253  2.122132  0.478539  3.475388  0.302254  ...  0.259459  1.956834   \n",
              "2085  2.006328  3.812641  2.159756  0.246238  ...  0.593444  1.551995   \n",
              "3900  0.807927  0.757049  3.170386  1.124121  ...  1.489706  2.728544   \n",
              "4666  2.990919  2.438957  3.287937  1.642361  ...  1.565336  1.510220   \n",
              "460   1.436209  0.473223  3.572704  0.717079  ...  0.266683  2.033299   \n",
              "\n",
              "           504       505       506       507       508       509       510  \\\n",
              "338   1.114262  0.779856  1.652471  0.326921  1.260001  1.834785  0.982471   \n",
              "4191  1.326782  0.951317  0.767415  0.734094  0.297562  4.567427  4.015381   \n",
              "2000  0.873747  0.523680  0.579251  0.763787  0.231753  2.854530  0.784511   \n",
              "4989  2.934217  2.101435  0.662123  0.779637  0.329928  2.556285  2.060014   \n",
              "4845  2.772453  1.588256  0.553294  1.165043  0.928831  3.515904  1.497891   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "4253  1.301193  5.171215  0.700102  1.314510  0.467656  3.209450  4.060209   \n",
              "2085  1.452943  2.396023  2.059243  1.407080  0.591397  1.037485  3.657440   \n",
              "3900  3.334295  1.660389  0.344655  1.022761  1.860433  1.060542  1.577378   \n",
              "4666  2.828382  1.659220  0.788920  2.494278  1.477300  2.348673  3.727552   \n",
              "460   2.407977  0.912128  0.864750  1.138418  1.234299  3.569176  1.418533   \n",
              "\n",
              "           511  \n",
              "338   0.643780  \n",
              "4191  1.085661  \n",
              "2000  1.215255  \n",
              "4989  0.602126  \n",
              "4845  1.511364  \n",
              "...        ...  \n",
              "4253  0.656931  \n",
              "2085  1.441381  \n",
              "3900  2.764105  \n",
              "4666  3.094683  \n",
              "460   1.386886  \n",
              "\n",
              "[6333 rows x 516 columns]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Merge the two DataFrames on the 'id' column\n",
        "df_test = pd.merge(file1, file2, on='ID').sort_values(by='ID')\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxzJNd4B8U4-",
        "outputId": "cbce96b3-25b0-4231-846a-9c7d4aec1765"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "338     ISIC_0000006\n",
              "4191    ISIC_0000011\n",
              "2000    ISIC_0000014\n",
              "4989    ISIC_0000018\n",
              "4845    ISIC_0000022\n",
              "            ...     \n",
              "4253    ISIC_0073240\n",
              "2085    ISIC_0073246\n",
              "3900    ISIC_0073247\n",
              "4666    ISIC_0073249\n",
              "460     ISIC_0073254\n",
              "Name: ID, Length: 6333, dtype: object"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ids = df_test[\"ID\"]\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AclIYDH88U4_",
        "outputId": "6e3ceaed-da87-4daf-d135-a0541dbf7af3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ID          0\n",
              "SEX         0\n",
              "AGE         0\n",
              "POSITION    0\n",
              "0           0\n",
              "           ..\n",
              "507         0\n",
              "508         0\n",
              "509         0\n",
              "510         0\n",
              "511         0\n",
              "Length: 516, dtype: int64"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_test['SEX'].fillna(fill_sex_for_test , inplace=True)\n",
        "df_test[\"AGE\"].fillna(fill_age_for_test, inplace=True)\n",
        "df_test['POSITION'].fillna(fill_position_for_test, inplace=True)\n",
        "df_test.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0FXPLcY8U4_",
        "outputId": "5bf33174-cfe4-46fe-919a-749b3b8d2471"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>SEX</th>\n",
              "      <th>AGE</th>\n",
              "      <th>POSITION</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>...</th>\n",
              "      <th>503</th>\n",
              "      <th>504</th>\n",
              "      <th>505</th>\n",
              "      <th>506</th>\n",
              "      <th>507</th>\n",
              "      <th>508</th>\n",
              "      <th>509</th>\n",
              "      <th>510</th>\n",
              "      <th>511</th>\n",
              "      <th>age_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>338</th>\n",
              "      <td>ISIC_0000006</td>\n",
              "      <td>female</td>\n",
              "      <td>25.0</td>\n",
              "      <td>posterior torso</td>\n",
              "      <td>0.833705</td>\n",
              "      <td>1.060439</td>\n",
              "      <td>1.023758</td>\n",
              "      <td>0.967986</td>\n",
              "      <td>1.432524</td>\n",
              "      <td>0.081379</td>\n",
              "      <td>...</td>\n",
              "      <td>2.054084</td>\n",
              "      <td>1.114262</td>\n",
              "      <td>0.779856</td>\n",
              "      <td>1.652471</td>\n",
              "      <td>0.326921</td>\n",
              "      <td>1.260001</td>\n",
              "      <td>1.834785</td>\n",
              "      <td>0.982471</td>\n",
              "      <td>0.643780</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4191</th>\n",
              "      <td>ISIC_0000011</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>0.520693</td>\n",
              "      <td>1.775505</td>\n",
              "      <td>1.454161</td>\n",
              "      <td>0.384557</td>\n",
              "      <td>2.993728</td>\n",
              "      <td>0.438422</td>\n",
              "      <td>...</td>\n",
              "      <td>1.722538</td>\n",
              "      <td>1.326782</td>\n",
              "      <td>0.951317</td>\n",
              "      <td>0.767415</td>\n",
              "      <td>0.734094</td>\n",
              "      <td>0.297562</td>\n",
              "      <td>4.567427</td>\n",
              "      <td>4.015381</td>\n",
              "      <td>1.085661</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2000</th>\n",
              "      <td>ISIC_0000014</td>\n",
              "      <td>male</td>\n",
              "      <td>35.0</td>\n",
              "      <td>posterior torso</td>\n",
              "      <td>0.410760</td>\n",
              "      <td>1.611643</td>\n",
              "      <td>0.729729</td>\n",
              "      <td>0.279183</td>\n",
              "      <td>1.355647</td>\n",
              "      <td>0.337431</td>\n",
              "      <td>...</td>\n",
              "      <td>1.830468</td>\n",
              "      <td>0.873747</td>\n",
              "      <td>0.523680</td>\n",
              "      <td>0.579251</td>\n",
              "      <td>0.763787</td>\n",
              "      <td>0.231753</td>\n",
              "      <td>2.854530</td>\n",
              "      <td>0.784511</td>\n",
              "      <td>1.215255</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4989</th>\n",
              "      <td>ISIC_0000018</td>\n",
              "      <td>male</td>\n",
              "      <td>30.0</td>\n",
              "      <td>posterior torso</td>\n",
              "      <td>1.127246</td>\n",
              "      <td>1.793517</td>\n",
              "      <td>0.763771</td>\n",
              "      <td>0.345557</td>\n",
              "      <td>1.504942</td>\n",
              "      <td>0.274724</td>\n",
              "      <td>...</td>\n",
              "      <td>2.109586</td>\n",
              "      <td>2.934217</td>\n",
              "      <td>2.101435</td>\n",
              "      <td>0.662123</td>\n",
              "      <td>0.779637</td>\n",
              "      <td>0.329928</td>\n",
              "      <td>2.556285</td>\n",
              "      <td>2.060014</td>\n",
              "      <td>0.602126</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4845</th>\n",
              "      <td>ISIC_0000022</td>\n",
              "      <td>female</td>\n",
              "      <td>55.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>0.315831</td>\n",
              "      <td>2.786491</td>\n",
              "      <td>1.038399</td>\n",
              "      <td>0.786178</td>\n",
              "      <td>1.367861</td>\n",
              "      <td>0.946043</td>\n",
              "      <td>...</td>\n",
              "      <td>2.975137</td>\n",
              "      <td>2.772453</td>\n",
              "      <td>1.588256</td>\n",
              "      <td>0.553294</td>\n",
              "      <td>1.165043</td>\n",
              "      <td>0.928831</td>\n",
              "      <td>3.515904</td>\n",
              "      <td>1.497891</td>\n",
              "      <td>1.511364</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4253</th>\n",
              "      <td>ISIC_0073240</td>\n",
              "      <td>female</td>\n",
              "      <td>45.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>1.006920</td>\n",
              "      <td>4.349065</td>\n",
              "      <td>2.122132</td>\n",
              "      <td>0.478539</td>\n",
              "      <td>3.475388</td>\n",
              "      <td>0.302254</td>\n",
              "      <td>...</td>\n",
              "      <td>1.956834</td>\n",
              "      <td>1.301193</td>\n",
              "      <td>5.171215</td>\n",
              "      <td>0.700102</td>\n",
              "      <td>1.314510</td>\n",
              "      <td>0.467656</td>\n",
              "      <td>3.209450</td>\n",
              "      <td>4.060209</td>\n",
              "      <td>0.656931</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2085</th>\n",
              "      <td>ISIC_0073246</td>\n",
              "      <td>male</td>\n",
              "      <td>80.0</td>\n",
              "      <td>anterior torso</td>\n",
              "      <td>0.589952</td>\n",
              "      <td>1.750752</td>\n",
              "      <td>2.006328</td>\n",
              "      <td>3.812641</td>\n",
              "      <td>2.159756</td>\n",
              "      <td>0.246238</td>\n",
              "      <td>...</td>\n",
              "      <td>1.551995</td>\n",
              "      <td>1.452943</td>\n",
              "      <td>2.396023</td>\n",
              "      <td>2.059243</td>\n",
              "      <td>1.407080</td>\n",
              "      <td>0.591397</td>\n",
              "      <td>1.037485</td>\n",
              "      <td>3.657440</td>\n",
              "      <td>1.441381</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3900</th>\n",
              "      <td>ISIC_0073247</td>\n",
              "      <td>female</td>\n",
              "      <td>85.0</td>\n",
              "      <td>head/neck</td>\n",
              "      <td>1.196528</td>\n",
              "      <td>0.874397</td>\n",
              "      <td>0.807927</td>\n",
              "      <td>0.757049</td>\n",
              "      <td>3.170386</td>\n",
              "      <td>1.124121</td>\n",
              "      <td>...</td>\n",
              "      <td>2.728544</td>\n",
              "      <td>3.334295</td>\n",
              "      <td>1.660389</td>\n",
              "      <td>0.344655</td>\n",
              "      <td>1.022761</td>\n",
              "      <td>1.860433</td>\n",
              "      <td>1.060542</td>\n",
              "      <td>1.577378</td>\n",
              "      <td>2.764105</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4666</th>\n",
              "      <td>ISIC_0073249</td>\n",
              "      <td>male</td>\n",
              "      <td>70.0</td>\n",
              "      <td>lower extremity</td>\n",
              "      <td>2.500076</td>\n",
              "      <td>3.883769</td>\n",
              "      <td>2.990919</td>\n",
              "      <td>2.438957</td>\n",
              "      <td>3.287937</td>\n",
              "      <td>1.642361</td>\n",
              "      <td>...</td>\n",
              "      <td>1.510220</td>\n",
              "      <td>2.828382</td>\n",
              "      <td>1.659220</td>\n",
              "      <td>0.788920</td>\n",
              "      <td>2.494278</td>\n",
              "      <td>1.477300</td>\n",
              "      <td>2.348673</td>\n",
              "      <td>3.727552</td>\n",
              "      <td>3.094683</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>460</th>\n",
              "      <td>ISIC_0073254</td>\n",
              "      <td>male</td>\n",
              "      <td>50.0</td>\n",
              "      <td>upper extremity</td>\n",
              "      <td>0.787703</td>\n",
              "      <td>2.927067</td>\n",
              "      <td>1.436209</td>\n",
              "      <td>0.473223</td>\n",
              "      <td>3.572704</td>\n",
              "      <td>0.717079</td>\n",
              "      <td>...</td>\n",
              "      <td>2.033299</td>\n",
              "      <td>2.407977</td>\n",
              "      <td>0.912128</td>\n",
              "      <td>0.864750</td>\n",
              "      <td>1.138418</td>\n",
              "      <td>1.234299</td>\n",
              "      <td>3.569176</td>\n",
              "      <td>1.418533</td>\n",
              "      <td>1.386886</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6333 rows × 517 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                ID     SEX   AGE         POSITION         0         1  \\\n",
              "338   ISIC_0000006  female  25.0  posterior torso  0.833705  1.060439   \n",
              "4191  ISIC_0000011  female  35.0  lower extremity  0.520693  1.775505   \n",
              "2000  ISIC_0000014    male  35.0  posterior torso  0.410760  1.611643   \n",
              "4989  ISIC_0000018    male  30.0  posterior torso  1.127246  1.793517   \n",
              "4845  ISIC_0000022  female  55.0  lower extremity  0.315831  2.786491   \n",
              "...            ...     ...   ...              ...       ...       ...   \n",
              "4253  ISIC_0073240  female  45.0  lower extremity  1.006920  4.349065   \n",
              "2085  ISIC_0073246    male  80.0   anterior torso  0.589952  1.750752   \n",
              "3900  ISIC_0073247  female  85.0        head/neck  1.196528  0.874397   \n",
              "4666  ISIC_0073249    male  70.0  lower extremity  2.500076  3.883769   \n",
              "460   ISIC_0073254    male  50.0  upper extremity  0.787703  2.927067   \n",
              "\n",
              "             2         3         4         5  ...       503       504  \\\n",
              "338   1.023758  0.967986  1.432524  0.081379  ...  2.054084  1.114262   \n",
              "4191  1.454161  0.384557  2.993728  0.438422  ...  1.722538  1.326782   \n",
              "2000  0.729729  0.279183  1.355647  0.337431  ...  1.830468  0.873747   \n",
              "4989  0.763771  0.345557  1.504942  0.274724  ...  2.109586  2.934217   \n",
              "4845  1.038399  0.786178  1.367861  0.946043  ...  2.975137  2.772453   \n",
              "...        ...       ...       ...       ...  ...       ...       ...   \n",
              "4253  2.122132  0.478539  3.475388  0.302254  ...  1.956834  1.301193   \n",
              "2085  2.006328  3.812641  2.159756  0.246238  ...  1.551995  1.452943   \n",
              "3900  0.807927  0.757049  3.170386  1.124121  ...  2.728544  3.334295   \n",
              "4666  2.990919  2.438957  3.287937  1.642361  ...  1.510220  2.828382   \n",
              "460   1.436209  0.473223  3.572704  0.717079  ...  2.033299  2.407977   \n",
              "\n",
              "           505       506       507       508       509       510       511  \\\n",
              "338   0.779856  1.652471  0.326921  1.260001  1.834785  0.982471  0.643780   \n",
              "4191  0.951317  0.767415  0.734094  0.297562  4.567427  4.015381  1.085661   \n",
              "2000  0.523680  0.579251  0.763787  0.231753  2.854530  0.784511  1.215255   \n",
              "4989  2.101435  0.662123  0.779637  0.329928  2.556285  2.060014  0.602126   \n",
              "4845  1.588256  0.553294  1.165043  0.928831  3.515904  1.497891  1.511364   \n",
              "...        ...       ...       ...       ...       ...       ...       ...   \n",
              "4253  5.171215  0.700102  1.314510  0.467656  3.209450  4.060209  0.656931   \n",
              "2085  2.396023  2.059243  1.407080  0.591397  1.037485  3.657440  1.441381   \n",
              "3900  1.660389  0.344655  1.022761  1.860433  1.060542  1.577378  2.764105   \n",
              "4666  1.659220  0.788920  2.494278  1.477300  2.348673  3.727552  3.094683   \n",
              "460   0.912128  0.864750  1.138418  1.234299  3.569176  1.418533  1.386886   \n",
              "\n",
              "      age_category  \n",
              "338              4  \n",
              "4191             6  \n",
              "2000             6  \n",
              "4989             5  \n",
              "4845            10  \n",
              "...            ...  \n",
              "4253             8  \n",
              "2085            15  \n",
              "3900            16  \n",
              "4666            13  \n",
              "460              9  \n",
              "\n",
              "[6333 rows x 517 columns]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the bins and labels for age categories\n",
        "bins = [ 0.,  5., 10., 15., 20., 25., 30., 35., 40., 45., 50., 55., 60.,\n",
        "       65., 70., 75., 80., 85., float('inf')]\n",
        "labels = list(range(len(bins)-1))\n",
        "\n",
        "# Create a new column 'age_category' by binning the 'age' column\n",
        "df_test['age_category'] = pd.cut(df_test['AGE'], bins=bins, labels=labels, include_lowest=True)\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9f6N4Y-Y8U5A",
        "outputId": "bc747c37-268b-4bb5-e381-38ef212e132c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['SEX', 'POSITION', '0', '1', '2', '3', '4', '5', '6', '7',\n",
              "       ...\n",
              "       '503', '504', '505', '506', '507', '508', '509', '510', '511',\n",
              "       'age_category'],\n",
              "      dtype='object', length=515)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Remove the 'ID' and 'age' columns\n",
        "df_test.drop(['ID', 'AGE'], axis=1, inplace=True)\n",
        "df_test.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "32cR3O138U5A"
      },
      "source": [
        "## Merging + encoding + unmerging\n",
        "This is important to get the same one hot encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a79ZPVip8U5A"
      },
      "outputs": [],
      "source": [
        "# Step 1: Concatenate the DataFrames vertically\n",
        "merged_df = pd.concat([X, X_valid, df_test], ignore_index=True)\n",
        "\n",
        "# Step 2: Apply one-hot encoding\n",
        "merged_df_encoded = pd.get_dummies(merged_df)\n",
        "\n",
        "# Step 3: Split the concatenated DataFrame back into original DataFrames\n",
        "num_rows_df1 = len(X)\n",
        "num_rows_df2 = len(X_valid)\n",
        "\n",
        "X_train_encoded = merged_df_encoded.iloc[:num_rows_df1]\n",
        "X_valid_encoded = merged_df_encoded.iloc[num_rows_df1:num_rows_df1+num_rows_df2]\n",
        "df_test_encoded = merged_df_encoded.iloc[num_rows_df1+num_rows_df2:]\n",
        "\n",
        "# If needed, you can reset index for each DataFrame\n",
        "X_train_encoded.reset_index(drop=True, inplace=True)\n",
        "X_valid_encoded.reset_index(drop=True, inplace=True)\n",
        "df_test_encoded.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "n1K7zm1U8U5B"
      },
      "source": [
        "## Dataloader for train-valid set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwdu2WQPwe6L"
      },
      "outputs": [],
      "source": [
        "X_train_encoded_numeric = X_train_encoded.values.astype(np.float32)\n",
        "y_numeric = y.values.astype(np.int64)-1\n",
        "trainset = TensorDataset(torch.tensor(X_train_encoded_numeric), torch.tensor(y_numeric))\n",
        "\n",
        "X_valid_encoded_numeric = X_valid_encoded.values.astype(np.float32)\n",
        "y_valid_numeric = y_valid.values.astype(np.int64)-1\n",
        "testset = TensorDataset(torch.tensor(X_valid_encoded_numeric),torch.tensor(y_valid_numeric))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZSS4NjxxMnk",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# Building MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "Tv1Lgqfh8U5C"
      },
      "source": [
        "## Defining the architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tWLK_siwe3U"
      },
      "outputs": [],
      "source": [
        "n_hidden_1 = 528 # 1st layer number of neurons\n",
        "n_hidden_2 = 256 # 2nd layer number of neurons\n",
        "n_hidden_3 = 128 # 3rd layer number of neurons\n",
        "\n",
        "n_input = len(X_train_encoded.columns) # input shape\n",
        "n_output = 8 #\n",
        "\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCPfx2atxZV6"
      },
      "outputs": [],
      "source": [
        "model_multi_layer_do = nn.Sequential(\n",
        "    nn.Linear(n_input,n_hidden_1),nn.ReLU(),nn.BatchNorm1d(n_hidden_1),nn.Dropout(dropout_rate),\n",
        "    nn.Linear(n_hidden_1,n_hidden_2),nn.ReLU(),nn.BatchNorm1d(n_hidden_2),nn.Dropout(dropout_rate),\n",
        "    nn.Linear(n_hidden_2,n_hidden_3),nn.ReLU(),nn.BatchNorm1d(n_hidden_3),nn.Dropout(dropout_rate),\n",
        "    nn.Linear(n_hidden_3,n_output))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5UT49x9xamd"
      },
      "outputs": [],
      "source": [
        "model_multi_layer_do = model_multi_layer_do.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "oy-B7Piy8U5F"
      },
      "source": [
        "## Defining the hyperparameters, loss function, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZktmcVc8xak7"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "n_epochs = 30\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czWPDG3rxajv"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(weight = weights)\n",
        "optimizer = optim.Adam(params=model_multi_layer_do.parameters(),lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uulhJ3Lyxago"
      },
      "outputs": [],
      "source": [
        "training_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True,drop_last=False)\n",
        "test_loader = DataLoader(testset,batch_size=batch_size,shuffle=False,drop_last=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UnpFFgxxaeV"
      },
      "outputs": [],
      "source": [
        "# Helper function: we monitor the accuracy during the training\n",
        "def vector_to_class(x):\n",
        "  y = torch.argmax(F.softmax(x,dim=1),axis=1)\n",
        "  return y\n",
        "\n",
        "def prediction_accuracy(predict,labels,weights=weights):\n",
        "  w= torch.tensor([weights[label] for label in labels])\n",
        "  accuracy = (w*(predict == labels)).sum()/(labels.shape[0])\n",
        "  return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "tAIeLc7b8U5G"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ww8kk9_xacH",
        "outputId": "0255b638-0ff2-402a-9d96-b4e8b63fde34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 156.14batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: Train Loss: 0.0073\n",
            "Epoch 0: Train Accuracy: 0.8446550965309143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 173.50batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Train Loss: 0.0029\n",
            "Epoch 1: Train Accuracy: 0.9396844506263733\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 161.01batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Train Loss: 0.0021\n",
            "Epoch 2: Train Accuracy: 0.9557973146438599\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 163.47batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3: Train Loss: 0.0016\n",
            "Epoch 3: Train Accuracy: 0.9679714441299438\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 174.10batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: Train Loss: 0.0017\n",
            "Epoch 4: Train Accuracy: 0.9665623307228088\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 170.90batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5: Train Loss: 0.0013\n",
            "Epoch 5: Train Accuracy: 0.9723213911056519\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6: 100%|████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 218.83batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6: Train Loss: 0.0013\n",
            "Epoch 6: Train Accuracy: 0.9723847508430481\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7: 100%|████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 177.91batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7: Train Loss: 0.0016\n",
            "Epoch 7: Train Accuracy: 0.9671098589897156\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8: 100%|████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 133.21batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8: Train Loss: 0.0014\n",
            "Epoch 8: Train Accuracy: 0.9702062606811523\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9: 100%|████████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 214.99batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9: Train Loss: 0.0020\n",
            "Epoch 9: Train Accuracy: 0.9583030343055725\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10: 100%|███████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 192.33batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10: Train Loss: 0.0009\n",
            "Epoch 10: Train Accuracy: 0.9789493083953857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11: 100%|███████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 154.92batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11: Train Loss: 0.0009\n",
            "Epoch 11: Train Accuracy: 0.9787910580635071\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12: 100%|███████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 164.99batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12: Train Loss: 0.0016\n",
            "Epoch 12: Train Accuracy: 0.9689878225326538\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13: 100%|███████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 168.43batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13: Train Loss: 0.0008\n",
            "Epoch 13: Train Accuracy: 0.9828656911849976\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14: 100%|███████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 160.39batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14: Train Loss: 0.0013\n",
            "Epoch 14: Train Accuracy: 0.9739301800727844\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15: 100%|███████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 174.05batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15: Train Loss: 0.0013\n",
            "Epoch 15: Train Accuracy: 0.9721973538398743\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16: 100%|███████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 172.40batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16: Train Loss: 0.0006\n",
            "Epoch 16: Train Accuracy: 0.987177848815918\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17: 100%|███████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 166.12batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17: Train Loss: 0.0013\n",
            "Epoch 17: Train Accuracy: 0.9736810922622681\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18: 100%|███████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 171.49batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18: Train Loss: 0.0008\n",
            "Epoch 18: Train Accuracy: 0.9812451004981995\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19: 100%|███████████████████████████████████████████████████████████████████| 238/238 [00:01<00:00, 173.53batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19: Train Loss: 0.0006\n",
            "Epoch 19: Train Accuracy: 0.9863420724868774\n"
          ]
        }
      ],
      "source": [
        "model_multi_layer_do.train()\n",
        "\n",
        "for epoch in range(0,n_epochs):\n",
        "  train_loss=0.0\n",
        "  all_labels = []\n",
        "  all_predicted = []\n",
        "\n",
        "  with tqdm(training_loader, unit=\"batch\") as tepoch:\n",
        "    for data, labels in tepoch:\n",
        "      tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "      # Put the data on device\n",
        "      data = data.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # forward and backward passes\n",
        "      y_predict = model_multi_layer_do(data)\n",
        "      loss = criterion (y_predict,labels)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Compute the loss\n",
        "      train_loss += loss.item()\n",
        "      # Store labels and class predictions\n",
        "      all_labels.extend(labels.tolist())\n",
        "      all_predicted.extend(vector_to_class(y_predict).tolist())\n",
        "\n",
        "  print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(training_loader.dataset)))\n",
        "  print(f'Epoch {epoch}: Train Accuracy: {prediction_accuracy(np.array(all_predicted),np.array(all_labels))}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ixd622jQxaZy",
        "outputId": "8bdba384-1e95-4cf9-921d-8e09793cc82e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████████████████████████████████████████████████████| 60/60 [00:00<00:00, 475.56batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Accuracy: tensor(0.6837)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model_multi_layer_do.eval()\n",
        "\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with tqdm(test_loader, unit=\"batch\") as tepoch:\n",
        "  for data, labels in tepoch:\n",
        "    all_labels.extend(labels.tolist())\n",
        "\n",
        "    data = data.to(device)\n",
        "    y_predict = model_multi_layer_do(data)\n",
        "    all_predicted.extend(vector_to_class(y_predict).tolist())\n",
        "\n",
        "test_accuracy = prediction_accuracy(np.array(all_predicted),np.array(all_labels))\n",
        "\n",
        "print(\"\\nTest Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTD9HAEXxlap"
      },
      "source": [
        "# Predictions for test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "ogTHTe-G8U5J"
      },
      "source": [
        "### dataloader afeter above Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygMZG01FxjB0"
      },
      "outputs": [],
      "source": [
        "X_test_encoded_numeric = df_test_encoded.values.astype(np.float32)\n",
        "\n",
        "testset = TensorDataset(torch.tensor(X_test_encoded_numeric), torch.tensor(np.zeros(len(df_test_encoded))))\n",
        "\n",
        "test_loader = DataLoader(testset,batch_size=batch_size,shuffle=False,drop_last=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayOj1VHZ8U5K"
      },
      "source": [
        "### Function to generate predictions for the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHZ0LVfA8U5K"
      },
      "outputs": [],
      "source": [
        "def generate_predictions(model, test_loader):\n",
        "    predictions = []\n",
        "    for images, _ in test_loader:\n",
        "        # Forward pass to get predictions\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        predicted = vector_to_class(outputs).tolist()\n",
        "        predictions.extend(predicted)\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Generate predictions for the test dataset\n",
        "predicted_classes = generate_predictions(model_multi_layer_do, test_loader)\n",
        "\n",
        "# Create a CSV file and write predictions to it\n",
        "csv_file = 'predictions_final_to_submit.csv'\n",
        "with open(csv_file, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['ID', 'CLASS'])  # Write header\n",
        "    for i in range(len(ids)):\n",
        "        image_name = ids.iloc[i]\n",
        "        writer.writerow([image_name, predicted_classes[i]+1])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "l1fAqwjrSkwf",
        "Ak-bINK18U28",
        "yvvUTHcd8U3N",
        "zwy3eCBrQxGW",
        "1Pz2KqjHO_aJ",
        "GE0Z6povY8An",
        "5rKwpidVWAq9",
        "W1s1wLsFaL7e",
        "bVmB37BAZHKH",
        "VoJZahNWbiTI",
        "1ldbCwhHbllH",
        "R1CoGAiOb7ro",
        "KYd468r8gr2K",
        "5nWZ9EoRZKpm",
        "8A9-j8HQiwCT",
        "7baBeWFxj0kJ",
        "l_UvJX-PjDyx",
        "Yf7qHCLdjGN5",
        "2ETgaHyEjoZi",
        "1rg2QFRa1g1F",
        "RtdjrS4Vjxxg",
        "8_8S7PkkkOkL",
        "OhC3E3jfkcfJ",
        "pxuPE1ZJkjXh",
        "5AjsW5YMmsbG",
        "VKxSXULMkv7S",
        "a3ohp1Gl8U4U",
        "UHly-rMjlPAl",
        "pqufskQf8U4h",
        "iQyjUJkDlUX5",
        "v-dtpH9ntYxj",
        "k4kg9-LEm5bo",
        "5Bl702tp8U4q",
        "wYcKAohXwvZX",
        "qFr-w4hG8U46",
        "32cR3O138U5A",
        "n1K7zm1U8U5B",
        "kZSS4NjxxMnk",
        "Tv1Lgqfh8U5C",
        "oy-B7Piy8U5F",
        "tAIeLc7b8U5G",
        "iTD9HAEXxlap",
        "ogTHTe-G8U5J",
        "ayOj1VHZ8U5K"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}